{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqGAN_headlines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogch/masterproject_gan/blob/master/SeqGAN_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sjo87PQzZyc4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Generation using SeqGAN\n",
        "\n",
        "This notebook generates news headlines using the Machine Learning technology GAN (Generative Adversarial Networks)."
      ]
    },
    {
      "metadata": {
        "id": "OhjZVsMcaMqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yo4no6KeYnM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67b03dbb-a6af-4899-fa1c-08755a41d788"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnmI7T2sZAkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1edfbe43-342d-4129-a610-2c530b5ec87d"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Masterproject\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Masterproject\n",
            "abcnews-date-text.csv  screenshots\n",
            "Graph\t\t       SeqGAN\n",
            "headlines.csv\t       SeqGAN_headlines_dataloading_experiments.ipynb\n",
            "news-headlines.db      SeqGAN_headlines.ipynb\n",
            "results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vRnegJVw-2YX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install required dependencies manually**"
      ]
    },
    {
      "metadata": {
        "id": "bS58gcTIfiCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a40cd89f-d4e0-4213-f011-eac8987aade1"
      },
      "cell_type": "code",
      "source": [
        "!pip install tflearn\n",
        "!pip install tqdm\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip\n",
        "!pip install tensorboardcolab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.11.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->tflearn) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     / 491kB 1.7MB/s\n",
            "Building wheels for collected packages: tqdm\n",
            "  Running setup.py bdist_wheel for tqdm ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-kpvk4bhi/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n",
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gDvlo_jb_A9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import required modules**"
      ]
    },
    {
      "metadata": {
        "id": "6tvM_ph_8-Xl",
        "colab_type": "code",
        "outputId": "e1d6202d-7304-4696-f0bf-7efa89f3d146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import datetime\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tflearn.data_utils import pad_sequences, to_categorical\n",
        "from tensorflow.contrib import slim\n",
        "from tqdm import tqdm, tnrange\n",
        "\n",
        "from tensorboardcolab import *"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4zlW6cONe9YE",
        "colab_type": "code",
        "outputId": "b918edb1-08be-443d-ecb5-1b9694e0d1f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JZEIBoWN44Dv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Al3MGHfT46pq",
        "colab_type": "code",
        "outputId": "ea957a90-5bda-427d-e55e-04f016860307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "df_real = pd.read_csv('headlines.csv', sep=',', usecols=['text', 'fake'])\n",
        "df_real = df_real.sample(frac=1)\n",
        "df_real = df_real[:50000]\n",
        "print(df_real.shape)\n",
        "df_real.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>856578</th>\n",
              "      <td>oberon abattoir re opens</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605514</th>\n",
              "      <td>ipswich flood victims lash out at insurers</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624582</th>\n",
              "      <td>mid north coast horse owners warned to keep</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314593</th>\n",
              "      <td>strong interest in roxby downs house land pack...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650814</th>\n",
              "      <td>missing man finds way to station</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  fake\n",
              "856578                           oberon abattoir re opens     0\n",
              "605514         ipswich flood victims lash out at insurers     0\n",
              "624582        mid north coast horse owners warned to keep     0\n",
              "314593  strong interest in roxby downs house land pack...     0\n",
              "650814                   missing man finds way to station     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "P1HQ85uyARIJ",
        "colab_type": "code",
        "outputId": "cb65e4a7-e8f3-4ea7-c03a-a3f33488bfc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "cell_type": "code",
      "source": [
        "df_fake = pd.DataFrame(columns=['text', 'fake'])\n",
        "print(df_fake.shape)\n",
        "df_fake.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, fake]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "UFCT682iAiwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Test-, Trainingset and Hyper-Parameter"
      ]
    },
    {
      "metadata": {
        "id": "mat90D4fAnPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General Hyper-Parameter\n",
        "BATCH_SIZE = 32\n",
        "VOCAB_SIZE = 2000 # default value (later dynamic assigned with length of word index)\n",
        "SEQ_LENGTH = 8 # average sequence length of the given sentences in the dataset\n",
        "TRAINING_SPLIT = 0.2\n",
        "TOTAL_EPOCH = 200\n",
        "\n",
        "# Discriminator Hyper-Parameter\n",
        "D_PRETRAIN_EPOCHS = 50\n",
        "D_EMB_SIZE = 100\n",
        "D_EMB_DIM = 64\n",
        "D_FILTER_SIZES = [2,3]\n",
        "D_NUM_CLASSES = 2\n",
        "D_NUM_FILTERS = 50\n",
        "\n",
        "# Generator Hyper-Parameter\n",
        "G_PRETRAIN_EPOCHS = 1000\n",
        "G_START_TOKEN = 0\n",
        "G_EMB_SIZE = 300\n",
        "G_EMB_DIM = 32\n",
        "G_HIDDEN_DIM = 32 # hidden state dimension of lstm cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dNP674HF22oU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_datasets(texts, labels, tokenizer=None):\n",
        "  if tokenizer is None:\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  word_index = tokenizer.word_index\n",
        "  text_seq = pad_sequences(sequences, maxlen=SEQ_LENGTH)\n",
        "    \n",
        "  labels = np.asarray(labels)\n",
        "\n",
        "  indices = np.arange(text_seq.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  text_seq = text_seq[indices]\n",
        "  labels = labels[indices]\n",
        "  test_size = int(TRAINING_SPLIT * text_seq.shape[0])\n",
        "  \n",
        "  X_train = text_seq[:test_size]\n",
        "  y_train = to_categorical(labels[:test_size], 2)\n",
        "  X_test = text_seq[test_size:]\n",
        "  y_test = labels[test_size:]\n",
        "  \n",
        "  return tokenizer, word_index, X_train, y_train, X_test, y_test\n",
        "  \n",
        "def load_fake_data():\n",
        "  texts = []\n",
        "  labels = []\n",
        "  \n",
        "  global df_fake\n",
        "  df_fake = df_fake.sample(frac=1)\n",
        "  \n",
        "  for row in zip(df_fake['text'], df_fake['fake']):\n",
        "    texts.append(row[0].strip())\n",
        "    labels.append(row[1])\n",
        "\n",
        "  tokenizer, word_index, X_train, y_train, X_test, y_test = get_datasets(texts, labels)\n",
        "  VOCAB_SIZE = len(word_index) # number of different words in the dataset\n",
        "  \n",
        "  return tokenizer, word_index, X_train, y_train, X_test, y_test\n",
        "\n",
        "def load_real_data():\n",
        "  texts = []\n",
        "  labels = []\n",
        "  \n",
        "  global df_real\n",
        "  df_real = df_real.sample(frac=1)\n",
        "  \n",
        "  for row in zip(df_real['text'], df_real['fake']):\n",
        "    texts.append(row[0].strip())\n",
        "    labels.append(row[1])\n",
        "\n",
        "  tokenizer, word_index, X_train, y_train, X_test, y_test = get_datasets(texts, labels)\n",
        "  VOCAB_SIZE = len(word_index) # number of different words in the dataset\n",
        "  \n",
        "  return tokenizer, word_index, X_train, y_train, X_test, y_test\n",
        "\n",
        "def load_mixed_data():\n",
        "  texts = []\n",
        "  labels = []\n",
        "  \n",
        "  df = pd.concat([df_real, df_fake])\n",
        "  df = df.sample(frac=1)\n",
        "\n",
        "  for row in zip(df['text'], df['fake']):\n",
        "    texts.append(row[0].strip())\n",
        "    labels.append(row[1])\n",
        "\n",
        "  tokenizer, word_index, X_train, y_train, X_test, y_test = get_datasets(texts, labels)\n",
        "  VOCAB_SIZE = len(word_index) # maybe +1 for padding? # number of different words in the dataset\n",
        "  \n",
        "  return tokenizer, word_index, X_train, y_train, X_test, y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qIo4O_PW1Hfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discriminator\n",
        "model for classifying sequences (here headlines) as real or fake.\n",
        "In this implementation the discriminative model uses following layers: \n",
        "1.   embedding layer\n",
        "2.   convolution layer\n",
        "3.   max-pooling layer\n",
        "4.   softmax layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k3UdGGRc5FfF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator:\n",
        "  def __init__(self, vocab_size, seq_length, emb_size, filter_sizes, num_classes, num_filters, learning_rate):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.seq_length = seq_length\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[None, self.seq_length], name='d_X_input')\n",
        "    self.y_input = tf.placeholder(tf.int32, shape=[None, self.num_classes], name='d_y_input')\n",
        "    self.dropout_keep_prob = tf.placeholder(tf.float32, name='d_dropout_keep_prob')\n",
        "    \n",
        "    # Keeping track of l2 regularization loss (optional)\n",
        "    self.l2_reg_lambda = 0.2\n",
        "    self.l2_loss = tf.constant(0.0)\n",
        "\n",
        "  def build_model(self):\n",
        "    with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):\n",
        "      self.embedding_layer = self.build_embedding_layer()\n",
        "      self.convolution_maxpool_layer = self.build_convolution_maxpool_layer()\n",
        "      self.scores, self.predictions = self.build_softmax_layer()\n",
        "\n",
        "      self.calc_mean_cross_entropy_loss()\n",
        "      self.calc_accuracy_and_cost()\n",
        "\n",
        "      self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
        "      \n",
        "      self.d_summary = tf.summary.merge([\n",
        "          tf.summary.scalar('d_loss', self.loss),\n",
        "          tf.summary.scalar('d_accuracy', self.accuracy)\n",
        "      ])\n",
        "        \n",
        "  def build_embedding_layer(self):\n",
        "    with tf.device('gpu:0'), tf.name_scope('d_embedding_layer'):\n",
        "      emb_matrix = tf.Variable(\n",
        "          initial_value=tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0), \n",
        "          name='d_emb_matrix'\n",
        "      )\n",
        "      emb_lookup = tf.nn.embedding_lookup(emb_matrix, self.X_input)\n",
        "      self.emb_chars_expand = tf.expand_dims(emb_lookup, -1)\n",
        "    \n",
        "  def build_convolution_maxpool_layer(self):\n",
        "    pooled_outputs = []\n",
        "    for filter_size in self.filter_sizes:\n",
        "      with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
        "        # Convolution Layer\n",
        "        filter_shape = [filter_size, self.emb_size, 1, self.num_filters]\n",
        "        W_filters = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
        "        b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name='b')\n",
        "        conv = tf.nn.conv2d(\n",
        "            input=self.emb_chars_expand,\n",
        "            filter=W_filters,\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='conv'\n",
        "        )\n",
        "        # Apply non-linearity - activation function\n",
        "        activation = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
        "        # Maxpooling over outputs\n",
        "        max_pooling = tf.nn.max_pool(\n",
        "            value=activation,\n",
        "            ksize=[1, self.seq_length-filter_size+1, 1, 1],\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='max_pooling'\n",
        "        )\n",
        "        pooled_outputs.append(max_pooling)\n",
        "              \n",
        "    # combine all the pooled features\n",
        "    self.num_filter_total = self.num_filters * len(self.filter_sizes)\n",
        "    h_pool = tf.concat(pooled_outputs, axis=3)\n",
        "    return tf.reshape(h_pool, [-1, self.num_filter_total])\n",
        "        \n",
        "  def build_softmax_layer(self): \n",
        "    with tf.name_scope('highway'):\n",
        "      self.h_highway = self.highway(\n",
        "          self.convolution_maxpool_layer, self.convolution_maxpool_layer.get_shape()[1], 1, 0\n",
        "      )\n",
        "\n",
        "    with tf.name_scope('dropout'):\n",
        "      self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
        "      \n",
        "    with tf.name_scope('softmax_output'):\n",
        "      W_softmax = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              [self.num_filter_total, self.num_classes], \n",
        "              stddev=0.1\n",
        "          ), name='W_softmax'\n",
        "      )\n",
        "      b_softmax = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name='b_softmax')\n",
        "            \n",
        "      self.l2_loss += tf.nn.l2_loss(W_softmax)\n",
        "      self.l2_loss += tf.nn.l2_loss(b_softmax)\n",
        "      \n",
        "      self.scores = tf.nn.xw_plus_b(self.h_drop, W_softmax, b_softmax, name='scores')\n",
        "      #self.scores = tf.matmul(self.convolution_maxpool_layer, W_softmax) + b_softmax\n",
        "      self.truth_prob = tf.nn.softmax(self.scores)\n",
        "      predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "      \n",
        "    return self.scores, predictions\n",
        "  \n",
        "  def calc_mean_cross_entropy_loss(self):\n",
        "    with tf.name_scope('mse_loss'):\n",
        "      losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=self.y_input)\n",
        "      self.loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
        "      \n",
        "  def calc_accuracy_and_cost(self):\n",
        "    with tf.name_scope('accuracy'):\n",
        "      correct_predictions = tf.equal(self.predictions, tf.argmax(self.y_input, 1))\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
        "      self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=self.y_input))\n",
        "      \n",
        "  def highway(self, input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n",
        "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
        "    t = sigmoid(Wy + b)\n",
        "    z = t * g(Wy + b) + (1 - t) * y\n",
        "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
        "    \"\"\"\n",
        "    with tf.variable_scope('highway'):\n",
        "      size = int(size)\n",
        "      for idx in range(num_layers):\n",
        "        g = f(slim.fully_connected(input_, size, scope='highway_lin_%d' % idx, activation_fn=None))\n",
        "        t = tf.sigmoid(slim.fully_connected(input_, size, scope='highway_gate_%d' % idx, activation_fn=None) + bias)\n",
        "\n",
        "        output = t * g + (1. - t) * input_\n",
        "        input_ = output\n",
        "        \n",
        "    return output\n",
        "  \n",
        "  def get_truth_prob(self, sess, X):\n",
        "    feed_dict = { self.X_input: X, self.dropout_keep_prob: 1.0 }\n",
        "        \n",
        "    return sess.run(self.truth_prob, feed_dict=feed_dict)\n",
        "        \n",
        "  def train(self, sess, X, y, dropout):\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    feed_dict = {\n",
        "        self.X_input: X,\n",
        "        self.y_input: y,\n",
        "        self.dropout_keep_prob: dropout\n",
        "    }\n",
        "    _, summary = sess.run([self.optimizer, self.d_summary], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B86yDcx3TC4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Discriminator model and train model"
      ]
    },
    {
      "metadata": {
        "id": "UnVPueM8TXwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "discriminator = Discriminator(VOCAB_SIZE, SEQ_LENGTH, D_EMB_SIZE, D_FILTER_SIZES, \n",
        "                              D_NUM_CLASSES, D_NUM_FILTERS, 0.001)\n",
        "discriminator.build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmjkZBeH557j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "\n",
        "LSTM with Reinforcement Learning for sequence generation."
      ]
    },
    {
      "metadata": {
        "id": "lcALithWy39s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib import rnn, layers, seq2seq, slim\n",
        "\n",
        "# inspired by https://www.oreilly.com/ideas/introduction-to-lstms-with-tensorflow\n",
        "# Decoder/Encoder approach from https://github.com/LA-JAMES/-Encoder-Decoder-Simple-Deep-LSTM-for-Tensorflow\n",
        "\n",
        "class Generator:\n",
        "  def __init__(self, batch_size, seq_length, vocab_size, emb_size, emb_dim, \n",
        "               hidden_dim, start_token, learning_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_length = seq_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.emb_dim = emb_dim\n",
        "    #self.hidden_dim = hidden_dim\n",
        "    #self.start_token = start_token\n",
        "    self.learning_rate = learning_rate\n",
        "    self.temperature = 1.0\n",
        "    self.grad_clip = 5.0\n",
        "    self.hidden_layer_sizes = [128]\n",
        "    self.hidden_layer_size = 128\n",
        "    \n",
        "    #self.X_input = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length], name='g_X_input')\n",
        "    self.encoder_X_input = tf.placeholder(dtype=tf.int32, \n",
        "                                          shape=[self.batch_size, self.seq_length], \n",
        "                                          name='g_encoder_X_input')\n",
        "    self.decoder_X_input = tf.placeholder(dtype=tf.int32, \n",
        "                                          shape=[self.batch_size, self.seq_length],\n",
        "                                          name='g_decoder_X_input')\n",
        "    self.y_input = tf.placeholder(tf.int32, shape=[None, self.vocab_size], name='g_y_input')\n",
        "    \n",
        "    #self.start_tokens = tf.Variable(tf.tile([self.start_token], [self.batch_size]), name='start_tokens')\n",
        "    self.decoder_lengths = tf.placeholder(tf.int32, shape=[self.batch_size], name='decoder_lengths')\n",
        "    self.dropout_keep_prob = tf.placeholder(tf.float32, name='g_dropout_keep_prob')\n",
        "    \n",
        "  def build_model(self):\n",
        "    with tf.variable_scope('generator', reuse=tf.AUTO_REUSE):\n",
        "      self.encoder_embedding_layer = self.build_embedding_layer(self.encoder_X_input)\n",
        "      self.decoder_embedding_layer = self.build_embedding_layer(self.decoder_X_input)\n",
        "\n",
        "      self.outputs, self.final_state = self.build_encoder_lstm_layers()\n",
        "      decoder_final_state, logits, self.prediction = self.build_decoder_lstm_layers()\n",
        "            \n",
        "      # PRETRAINING\n",
        "      \"\"\"self.pretrain_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "      self.pretrain_calc_loss()\n",
        "      self.pretrain_operation = slim.learning.create_train_op(\n",
        "          self.pretrain_loss, self.pretrain_optimizer, clip_gradient_norm=5.0\n",
        "      )\"\"\"\n",
        "      \n",
        "      # TRAINING\n",
        "      #self.predictions, self.rewards, loss = self.calc_loss_and_reward()\n",
        "      #accuracy = self.calc_accuracy()\n",
        "      #self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
        "      \"\"\"self.train_operation = slim.learning.create_train_op(\n",
        "          loss, self.optimizer, clip_gradient_norm=self.grad_clip\n",
        "      )\"\"\"\n",
        "                  \n",
        "      # SUMMARY\n",
        "      \"\"\"self.g_pretrain_summary = tf.summary.scalar(\n",
        "          'g_pretrain_loss', pretrain_loss\n",
        "      )\"\"\"\n",
        "      \"\"\"self.g_train_summary = tf.summary.merge([\n",
        "          tf.summary.scalar('g_loss', loss),\n",
        "          tf.summary.scalar('g_accuracy', accuracy)\n",
        "      ])\"\"\"\n",
        "        \n",
        "  def build_embedding_layer(self, X_input):\n",
        "    with tf.name_scope('g_embedding_layer'):\n",
        "      emb_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0))\n",
        "      emb_lookup = tf.nn.embedding_lookup(emb_matrix, X_input)\n",
        "      \n",
        "    return emb_lookup\n",
        "      \n",
        "  def build_encoder_lstm_layers(self):\n",
        "    with tf.name_scope('g_encoder_lstm_layers'):\n",
        "      layers = [rnn.LSTMCell(layer_size) for layer_size in self.hidden_layer_sizes]\n",
        "      dropouts = [rnn.DropoutWrapper(layer, output_keep_prob=self.dropout_keep_prob) for layer in layers]\n",
        "      cell = rnn.MultiRNNCell(dropouts) # , state_is_tuple=True)?\n",
        "      \n",
        "      initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
        "      outputs, final_state = tf.nn.dynamic_rnn(cell, self.encoder_embedding_layer, initial_state=initial_state)\n",
        "      \n",
        "    return outputs, final_state    \n",
        "      \n",
        "  def build_decoder_lstm_layers(self):\n",
        "    with tf.name_scope('g_decoder_lstm_layers'):\n",
        "      layers = [rnn.LSTMCell(layer_size) for layer_size in self.hidden_layer_sizes]\n",
        "      dropouts = [rnn.DropoutWrapper(layer, output_keep_prob=self.dropout_keep_prob) for layer in layers]\n",
        "      cell = rnn.MultiRNNCell(dropouts) # , state_is_tuple=True)?\n",
        "      \n",
        "      initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
        "      outputs, final_state = tf.nn.dynamic_rnn(cell, self.encoder_embedding_layer, initial_state=self.final_state)\n",
        "      \n",
        "      output = tf.reshape(outputs, [-1, self.hidden_layer_sizes[0]])\n",
        "      \n",
        "      W2 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[0], self.vocab_size]), dtype=tf.float32)\n",
        "      b2 = tf.Variable(tf.zeros([1, self.vocab_size]), dtype=tf.float32)\n",
        "      \n",
        "      logits = tf.matmul(output, W2) + b2 # broadcasted addition\n",
        "      prediction = tf.nn.softmax(logits)\n",
        "      \n",
        "    return final_state, logits, prediction\n",
        "  \n",
        "  def pretrain_calc_loss(self):\n",
        "    # PRETRAINING\n",
        "    # all tokens from given sequence\n",
        "    logit = final_outputs.rnn_output\n",
        "    pretrain_loss = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.X_input, logits=logit)\n",
        "    )\n",
        "    \n",
        "    return pretrain_loss\n",
        "    \n",
        "  def calc_loss_and_reward(self):\n",
        "    # add weights and biases and other way of calculating predictions like here: http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/\n",
        "    predictions = layers.fully_connected(self.outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
        "    rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.seq_length], name='rewards')\n",
        "    # from paper and implementation of SeqGAN\n",
        "    loss = -tf.reduce_sum(\n",
        "        tf.reduce_sum(\n",
        "            tf.one_hot(tf.to_int32(tf.reshape(self.encoder_X_input, [-1])), self.emb_dim, 1.0, 0.0) * \n",
        "            tf.log(tf.clip_by_value(tf.reshape(predictions, [-1, self.emb_dim]), 1e-20, 1.0)), 1\n",
        "        ) * tf.reshape(rewards, [-1])\n",
        "    )\n",
        "    \n",
        "    return predictions, rewards, loss\n",
        "    \n",
        "  def calc_accuracy(self):\n",
        "    # x or y???\n",
        "    correct_prediction = tf.equal(tf.argmax(self.predictions, 1), tf.argmax(self.y_input, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    \n",
        "    return accuracy\n",
        "  \n",
        "  def generate(self, sess, given_tokens, dropout=0.75):\n",
        "    feed_dict = { self.encoder_X_input: given_tokens, self.dropout_keep_prob: dropout }\n",
        "    outputs = sess.run([self.prediction], feed_dict=feed_dict)\n",
        "    \n",
        "    return outputs\n",
        "    \n",
        "  \"\"\"def sample(output, temperature=1.0):\n",
        "    output = np.log(output) / self.temperature\n",
        "    output = np.exp(output) / np.sum(np.exp(output))\n",
        "    r = random.random() # range: [0,1)\n",
        "    total = 0.0\n",
        "    for i in range(len(output)):\n",
        "      total += output[i]\n",
        "      if total>r:\n",
        "        return i\n",
        "    return len(output)-1\"\"\"\n",
        "    \n",
        "  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "  # REINFORCEMENT LEARNING\n",
        "  def rollout(self, sess, given_tokens, keep_steps=0, with_probs=False):\n",
        "    feed_dict = { self.X_input: given_tokens }\n",
        "    if with_probs:\n",
        "      output_tensors = [self.outputs[keep_steps], self.predictions[keep_steps]]\n",
        "    else:\n",
        "      output_tensors = self.outputs[keep_steps]\n",
        "      \n",
        "    return sess.run(output_tensors, feed_dict=feed_dict)\n",
        "  \n",
        "  def get_reward(self, sess, given_tokens, rollout_num, dis):\n",
        "    rewards = []\n",
        "    for i in range(rollout_num):\n",
        "      for keep_num in range(1, self.seq_length):\n",
        "        data = given_tokens[:, 0:1]\n",
        "        # Markov Chain Sample / Monte Carlo Sample??\n",
        "        mc_sample = self.rollout(sess, given_tokens, keep_steps=keep_num)\n",
        "        truth_prob = dis.get_truth_prob(sess, mc_sample)\n",
        "        pred = np.array([item[1] for item in truth_prob])\n",
        "        if i == 0:\n",
        "          rewards.append(pred)\n",
        "        else:\n",
        "          rewards[keep_num-1] += pred\n",
        "          \n",
        "      # for last token\n",
        "      truth_pred = dis.get_truth_prob(sess, given_tokens)\n",
        "      pred = np.array([item[1] for item in truth_prob])\n",
        "      if i ==0:\n",
        "        rewards.append(pred)\n",
        "      else:\n",
        "        rewards[self.seq_length-1] += pred\n",
        "        \n",
        "    rewards = np.transpose(np.array(rewards)) / (1.0 * rollout_num)\n",
        "    return rewards\n",
        "  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "  \n",
        "  def pretrain(self, sess, given_tokens, dropout=0.75):\n",
        "    feed_dict = { self.X_input: given_tokens, self.dropout_keep_prob: dropout }\n",
        "    sess.run(self.pretrain_operation, feed_dict=feed_dict)\n",
        "    _, summary = sess.run([self.pretrain_operation, self.g_pretrain_summary], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary\n",
        "  \n",
        "  def train(self, sess, given_tokens, rewards, dropout=0.75):\n",
        "    feed_dict = { self.X_input: given_tokens, self.rewards: rewards, self.dropout_keep_prob: dropout }\n",
        "    _, summary = sess.run([self.optimizer, self.g_train_summary], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V03XEe_uVdwq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Generator model"
      ]
    },
    {
      "metadata": {
        "id": "PW7Hyz2J6jro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = Generator(BATCH_SIZE, SEQ_LENGTH, VOCAB_SIZE, G_EMB_SIZE, \n",
        "                      G_EMB_DIM, G_HIDDEN_DIM, G_START_TOKEN, 0.0001)\n",
        "\n",
        "generator.build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uccb8c0sViK5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Start Adversarial Training"
      ]
    },
    {
      "metadata": {
        "id": "hN-drlXj1Kyj",
        "colab_type": "code",
        "outputId": "97878a3c-3ed3-40cd-dc9c-d1a7b4b859e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "tbc=TensorBoardColab()\n",
        "\n",
        "LOG_DIR = '/tmp/log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://26de22d7.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ymHD3nwRtS7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_text(tokenizer, samples):\n",
        "  reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "  \n",
        "  # Function takes a tokenized sentence and returns the words\n",
        "  def sequence_to_text(samples):\n",
        "      # Looking up words in dictionary\n",
        "      words = [reverse_word_map.get(letter) for letter in samples]\n",
        "      return words\n",
        "\n",
        "  # Creating texts \n",
        "  samples_text = list(map(sequence_to_text, samples))\n",
        "  return samples_text\n",
        "\n",
        "def add_fake_samples(tokenizer, samples):\n",
        "  #samples_text = get_text(tokenizer, samples)\n",
        "  #print(samples_text)\n",
        "  samples_text = samples\n",
        "  \n",
        "  headlines = []\n",
        "  for item in samples_text:\n",
        "    headline = ''\n",
        "    for word in item:\n",
        "      headline += '%s ' % word\n",
        "\n",
        "    global df_fake\n",
        "    df_fake = df_fake.append({'text' : headline , 'fake' : 1} , ignore_index=True)\n",
        "    \n",
        "def generate_text(prediction, word_index, batch_size=BATCH_SIZE, length=SEQ_LENGTH, vocab_size=VOCAB_SIZE):\n",
        "  idx_to_word = [word for idx, word in enumerate(word_index)]    \n",
        "  batch_softmax = np.reshape(prediction, [batch_size, length, vocab_size])\n",
        "  batch_sentence = []\n",
        "\n",
        "  for sequence in batch_softmax:\n",
        "    word_sequence = ''\n",
        "    for char in sequence:\n",
        "      vector_position = np.argmax(char)\n",
        "      y_word = idx_to_word[vector_position]\n",
        "      if y_word != 'ZERO':\n",
        "        word_sequence = word_sequence + y_word + ' '\n",
        "      else:\n",
        "        word_sequence = word_sequence + ''\n",
        "      batch_sentence.append(word_sequence)\n",
        "\n",
        "  return batch_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GbdSazLhVllp",
        "colab_type": "code",
        "outputId": "971bd64a-e22d-458a-adc1-0ae3b28cc600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1282
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  print('start GAN training at', datetime.datetime.now())\n",
        "  writer = tbc.get_writer()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  \"\"\"# PRETRAINING GENERATOR\n",
        "  for epoch in range(G_PRETRAIN_EPOCHS):\n",
        "    # seq_length -1 und von 1 anfangen\n",
        "    tokenizer, word_index, X_train, y_train, X_test, y_test = load_mixed_data()\n",
        "    summary = generator.pretrain(sess, X_train[:BATCH_SIZE])\n",
        "    writer.add_summary(summary, epoch)\n",
        "    \n",
        "  # PRETRAIN DISCRIMINATOR\n",
        "  for eoch in range(D_PRETRAIN_EPOCHS):\n",
        "    fake_samples = generator.generator(sess)\n",
        "    translate_samples(tokenizer, fake_samples, df_fake)\n",
        "    tokenizer, word_index, X_train, y_train, X_test, y_test = load_data()\n",
        "    \n",
        "    for _ in range(3):\n",
        "      discriminator.train(sess, X_train, y_train, 5, BATCH_SIZE, .001)\"\"\"\n",
        "\n",
        "  # ADVERSARIAL TRAINING\n",
        "  for epoch in tnrange(TOTAL_EPOCH, desc='gan_epoch_loop'):\n",
        "    tokenizer, word_index, X_train, y_train, X_test, y_test = load_real_data()\n",
        "    # train generator for one step\n",
        "    for it in range(1):\n",
        "      outputs = generator.generate(sess, X_train[:BATCH_SIZE])\n",
        "      fake_samples = generate_text(outputs, tokenizer.word_index)\n",
        "      print(fake_samples[-1])\n",
        "      add_fake_samples(tokenizer, fake_samples)\n",
        "      #rewards = generator.get_reward(sess, fake_samples, 16, discriminator)\n",
        "      #summary = generator.train(sess, fake_samples, rewards)\n",
        "    \n",
        "    #writer.add_summary(summary, epoch)\n",
        "    \n",
        "    for _ in tnrange(5, desc='gen_train_loop'):\n",
        "      tokenizer, word_index, X_train, y_train, X_test, y_test = load_real_data()\n",
        "      outputs = generator.generate(sess, X_train[:BATCH_SIZE])\n",
        "      fake_samples = generate_text(outputs, tokenizer.word_index)\n",
        "      print(fake_samples[-1])\n",
        "      add_fake_samples(tokenizer, fake_samples)\n",
        "      \n",
        "      for _ in tnrange(3, desc='dis_train_loop'):\n",
        "        tokenizer, word_index, X_train, y_train, X_test, y_test = load_mixed_data()\n",
        "        summary = discriminator.train(sess, X_train[:BATCH_SIZE], y_train[:BATCH_SIZE], 0.4)\n",
        "        \n",
        "    #print(df_fake.tail(2))\n",
        "    writer.add_summary(summary, epoch)\n",
        "    \n",
        "    \"\"\"tokenizer, word_index, X_real_train, y_real_train, X_real_test, y_real_test = load_real_data()\n",
        "    summary = sess.run(generator.image_summary, feed_dict={ generator.given_tokens: X_real_train[:BATCH_SIZE] })\n",
        "    writer.add_summary(summary, epoch)\"\"\"\n",
        "    \n",
        "  print('finish GAN training at', datetime.datetime.now())"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start GAN training at 2018-12-22 23:40:36.890115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gan_epoch_loop</span><progress style='margin:2px 4px;description_width:initial;' max='200' value='1'></progress>  0% 1/200 [00:53&lt;2:58:41, 53.88s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "display crocs arrests possible warriors display display display \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gen_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='5' value='5'></progress>100% 5/5 [00:51&lt;00:00, 10.22s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "east his toowoomba axe coroner meet meet chris \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:07&lt;00:00,  2.53s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "remain fail peace smoke eagles demands overhaul enough \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:07&lt;00:00,  2.43s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "rio education 40 timber welfare students november video \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:07&lt;00:00,  2.42s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "basin soon palm double officers england government orange \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:07&lt;00:00,  2.45s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "worker service closes flee hold surf prison league \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:07&lt;00:00,  2.45s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "man mps agenda fraser bombing bombing extra agenda \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gen_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='5' value='4'></progress> 80% 4/5 [00:42&lt;00:10, 10.58s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "teen challenges challenges urges urges flu victim philippines \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:07&lt;00:00,  2.49s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "fatal royal cent pilot exports have forward worker \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:07&lt;00:00,  2.64s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "training wales dont wales wales rio wales opening \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:07&lt;00:00,  2.52s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "supply potential supply mixed blue zealand zealand supply \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:07&lt;00:00,  2.65s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "their delay townsville fatal urges appeals road appeals \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='1'></progress> 33% 1/3 [00:02&lt;00:04,  2.48s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-08278a699db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtnrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dis_train_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mixed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-53c0e555968c>\u001b[0m in \u001b[0;36mload_mixed_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m   \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m   \u001b[0mVOCAB_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# maybe +1 for padding? # number of different words in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-53c0e555968c>\u001b[0m in \u001b[0;36mget_datasets\u001b[0;34m(texts, labels, tokenizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    220\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtranslate_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtranslate_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2TgllHrMIdaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "print(df_fake.tail())\n",
        "\n",
        "count = 1\n",
        "saved = False\n",
        "while saved != True:\n",
        "  filename = 'generated_headlines_%d.csv' % count\n",
        "  filepath = Path('./results/%s' % filename)\n",
        "  \n",
        "  if filepath.exists():\n",
        "    count = count+1\n",
        "  else:\n",
        "    df_fake.to_csv(filepath, sep='\\t', encoding='utf-8')\n",
        "    saved = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nNXWnagxP-8Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}