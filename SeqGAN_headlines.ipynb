{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqGAN_headlines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogch/masterproject_gan/blob/master/SeqGAN_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sjo87PQzZyc4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Generation using GAN\n",
        "\n",
        "This notebook generates news headlines using the Machine Learning technology GAN (Generative Adversarial Networks)."
      ]
    },
    {
      "metadata": {
        "id": "OhjZVsMcaMqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yo4no6KeYnM3",
        "colab_type": "code",
        "outputId": "bd66c57f-82a0-4b00-9556-bc6ca7b26946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnmI7T2sZAkv",
        "colab_type": "code",
        "outputId": "6e908a11-5377-42ed-d1df-e9320960fd4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Masterproject\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Masterproject\n",
            "abcnews-date-text.csv  news-headlines.db  SeqGAN_headlines.ipynb\n",
            "headlines.csv\t       SeqGAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gDvlo_jb_A9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import required modules**"
      ]
    },
    {
      "metadata": {
        "id": "6tvM_ph_8-Xl",
        "colab_type": "code",
        "outputId": "2864d276-430e-47f4-afe6-bff6fc6f5b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vRnegJVw-2YX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install required dependencies manually**"
      ]
    },
    {
      "metadata": {
        "id": "t8rLBesy-11S",
        "colab_type": "code",
        "outputId": "75fc911e-f4c2-4db2-ea77-e3b55aa2534d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tflearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/ec/e9ce1b52e71f6dff3bd944f020cef7140779e783ab27512ea7c7275ddee5/tflearn-0.3.2.tar.gz (98kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.11.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->tflearn) (0.46)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Running setup.py bdist_wheel for tflearn ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d0/f6/69/0ef3ee395aac2e5d15d89efd29a9a216f3c27767b43b72c006\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.3.2\n",
            "Collecting highway.py\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/35/e5a7a40f476701591f7d177dbf1e59d3ed292ca30ca3d27634519afb1783/highway.py-0.2.2.tar.gz\n",
            "Collecting websockets==3.4 (from highway.py)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/3a/2c3a5b2c65179851e80d4acae30cffb2610a8740a8edb2afbeaa564283f8/websockets-3.4-cp36-cp36m-manylinux1_x86_64.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: ujson==1.35 in /usr/local/lib/python3.6/dist-packages (from highway.py) (1.35)\n",
            "Building wheels for collected packages: highway.py\n",
            "  Running setup.py bdist_wheel for highway.py ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7c/7e/3b/9957539403dd1f0e7e7073722e2c5b3f6ff6e7e90e6f27e0d1\n",
            "Successfully built highway.py\n",
            "Installing collected packages: websockets, highway.py\n",
            "Successfully installed highway.py-0.2.2 websockets-3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JZEIBoWN44Dv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Al3MGHfT46pq",
        "colab_type": "code",
        "outputId": "b491cf9d-3c04-4bee-c990-810459779fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"headlines.csv\", sep=',',index_col = \"id\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1103665, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20030219</td>\n",
              "      <td>aba decides against community broadcasting lic...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20030219</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20030219</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    publish_date                                               text  fake\n",
              "id                                                                       \n",
              "1       20030219  aba decides against community broadcasting lic...     0\n",
              "2       20030219     act fire witnesses must be aware of defamation     0\n",
              "3       20030219     a g calls for infrastructure protection summit     0\n",
              "4       20030219           air nz staff in aust strike for pay rise     0\n",
              "5       20030219      air nz strike to affect australian travellers     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "UFCT682iAiwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Test-, Trainingset and Hyper-Parameter for the Discriminator"
      ]
    },
    {
      "metadata": {
        "id": "mat90D4fAnPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], \n",
        "    df['fake'], \n",
        "    test_size=0.2, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Discriminator Hyper-Parameter\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = 5000 # 20\n",
        "SEQ_LENGTH = 100 # 20\n",
        "EMB_SIZE = 100\n",
        "EMB_DIM = 64\n",
        "FILTER_SIZES = [2,3]\n",
        "NUM_CLASSES = 2\n",
        "NUM_FILTERS = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B86yDcx3TC4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialize Discriminator and build model"
      ]
    },
    {
      "metadata": {
        "id": "UnVPueM8TXwc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30a8a663-5cc2-4e3f-a1a9-491d27f34670"
      },
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator(VOCAB_SIZE, SEQ_LENGTH, EMB_SIZE, \n",
        "                              FILTER_SIZES, NUM_CLASSES, NUM_FILTERS)\n",
        "\n",
        "discriminator.build_model()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"cross_entropy_loss_1/add:0\", shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qIo4O_PW1Hfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discriminator\n",
        "model for classifying sequences (here headlines) as real or fake.\n",
        "In this implementation the discriminative model uses following layers: \n",
        "1.   embedding layer\n",
        "2.   convolution layer\n",
        "3.   max-pooling layer\n",
        "4.   softmax layer"
      ]
    },
    {
      "metadata": {
        "id": "nJlWqPe41GAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator:\n",
        "  def __init__(self, vocab_size, seq_length, emb_size, filter_sizes, num_classes, num_filters):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.seq_length = seq_length\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "\n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[None, self.seq_length], name='X_input')\n",
        "    self.y_input = tf.placeholder(tf.float32, shape=[None, self.num_classes], name='y_input')\n",
        "    #self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
        "    \n",
        "    # Keeping track of l2 regularization loss (optional)\n",
        "    self.l2_reg_lambda = 0.0\n",
        "    self.l2_loss = tf.constant(0.0)\n",
        "\n",
        "  def build_model(self):\n",
        "    self.embedding_layer = self.build_embedding_layer()\n",
        "    self.convolution_maxpool_layer = self.build_convolution_maxpool_layer()\n",
        "    self.scores, self.predictions = self.build_softmax_layer()\n",
        "    \n",
        "    self.calc_mean_cross_entropy_loss()\n",
        "        \n",
        "  def build_embedding_layer(self):\n",
        "    with tf.device('cpu:0'), tf.name_scope('embedding_layer'):\n",
        "      W_emb = tf.Variable(\n",
        "          initial_value=tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0), \n",
        "          name='W'\n",
        "      )\n",
        "      emb_chars = tf.nn.embedding_lookup(W_emb, self.X_input)\n",
        "      self.emb_chars_expand = tf.expand_dims(emb_chars, -1)\n",
        "    \n",
        "  def build_convolution_maxpool_layer(self):\n",
        "    pooled_outputs = []\n",
        "    for filter_size in self.filter_sizes:\n",
        "      with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
        "        # Convolution Layer\n",
        "        filter_shape = [filter_size, self.emb_size, 1, self.num_filters]\n",
        "        W_filters = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
        "        b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name='b')\n",
        "        conv = tf.nn.conv2d(\n",
        "            input=self.emb_chars_expand, #input=self.embedding_layer\n",
        "            filter=W_filters,\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='conv'\n",
        "        )\n",
        "        # Apply non-linearity - activation function\n",
        "        activation = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
        "        # Maxpooling over outputs\n",
        "        max_pooling = tf.nn.max_pool(\n",
        "            value=activation,\n",
        "            ksize=[1, self.seq_length-filter_size+1, 1, 1],\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='max_pooling'\n",
        "        )\n",
        "        pooled_outputs.append(max_pooling)\n",
        "        \n",
        "    concat_outputs = tf.concat(pooled_outputs, axis=3)\n",
        "    self.final_output_length = self.num_filters * len(pooled_outputs) # or sum(self.num_filters)?\n",
        "      \n",
        "    return tf.reshape(concat_outputs, [-1, self.final_output_length])\n",
        "        \n",
        "  def build_softmax_layer(self):\n",
        "    with tf.name_scope('softmax_output'):\n",
        "      W_softmax = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              [self.final_output_length, self.num_classes], \n",
        "              stddev=0.1\n",
        "          ), name='W'\n",
        "      )\n",
        "      b_softmax = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name='b')\n",
        "      \n",
        "      #h_highway = self.highway(self.convolution_maxpool_layer, self.convolution_maxpool_layer.get_shape()[1], 1, 0)\n",
        "      #h_dropout = tf.nn.dropout(h_highway, self.dropout_keep_prob)\n",
        "      \n",
        "      self.l2_loss += tf.nn.l2_loss(W_softmax)\n",
        "      self.l2_loss += tf.nn.l2_loss(b_softmax)\n",
        "      \n",
        "      #self.scores = tf.nn.xw_plus_b(h_dropout, W_softmax, b_softmax, name='scores')\n",
        "      self.scores = tf.matmul(self.convolution_maxpool_layer, W_softmax) + b_softmax\n",
        "      predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "      \n",
        "    return self.scores, predictions\n",
        "  \n",
        "  def calc_mean_cross_entropy_loss(self):\n",
        "    with tf.name_scope('cross_entropy_loss'):\n",
        "      losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.y_input)\n",
        "      self.loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sePiLqVd1XWy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Discriminator"
      ]
    },
    {
      "metadata": {
        "id": "BiaBSbXA1f-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    def train(self, X, y, nb_epochs, batch_size=32, learning_rate=.001):\n",
        "\n",
        "\n",
        "        # Evaluate model\n",
        "        correct_pred = tf.equal(self.predictions, tf.argmax(self.y_input, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, 'float'))\n",
        "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.y_input))\n",
        "\n",
        "        with tf.name_scope('loss'):\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "        init = tf.initialize_all_variables()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            epoch_i = 0\n",
        "\n",
        "            while epoch_i < nb_epochs:\n",
        "                batch_i = 0\n",
        "                batch_losses = []\n",
        "                batch_accs = []\n",
        "\n",
        "                for i in range(batch_size, X.shape[0], batch_size):\n",
        "                    X_batch, y_batch = X[batch_i:i], y[batch_i:i]\n",
        "\n",
        "                    sess.run(optimizer, feed_dict={\n",
        "                        self.X_input: X_batch,\n",
        "                        self.y_input: y_batch\n",
        "                    })\n",
        "\n",
        "                    loss, acc = sess.run([cost, accuracy], feed_dict={\n",
        "                        self.X_input: X_batch,\n",
        "                        self.y_input: y_batch\n",
        "                    })\n",
        "\n",
        "                    batch_accs.append(acc)\n",
        "                    batch_losses.append(loss)\n",
        "\n",
        "                    batch_i = i\n",
        "                print('Epoch: {} loss: {:.6f} acc: {:.6f}'.format(epoch_i + 1, mean(batch_losses), mean(batch_accs)))\n",
        "\n",
        "                epoch_i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}