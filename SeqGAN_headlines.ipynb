{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqGAN_headlines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogch/masterproject_gan/blob/master/SeqGAN_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sjo87PQzZyc4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Generation using SeqGAN\n",
        "\n",
        "This notebook generates news headlines using the Machine Learning technology GAN (Generative Adversarial Networks)."
      ]
    },
    {
      "metadata": {
        "id": "OhjZVsMcaMqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yo4no6KeYnM3",
        "colab_type": "code",
        "outputId": "d25afe71-80ad-4c15-f87d-7346f5471fb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnmI7T2sZAkv",
        "colab_type": "code",
        "outputId": "25833fd9-c1ac-4fd4-bf5a-4d9c313f764f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Masterproject\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Masterproject\n",
            "abcnews-date-text.csv  screenshots\n",
            "Graph\t\t       SeqGAN\n",
            "headlines.csv\t       SeqGAN_headlines_dataloading_experiments.ipynb\n",
            "news-headlines.db      SeqGAN_headlines.ipynb\n",
            "results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vRnegJVw-2YX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install required dependencies manually**"
      ]
    },
    {
      "metadata": {
        "id": "bS58gcTIfiCx",
        "colab_type": "code",
        "outputId": "edec85e7-8b20-4bc2-bc02-9668db635907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tflearn\n",
        "!pip install tqdm\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip\n",
        "!pip install tensorboardcolab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.11.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->tflearn) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     / 614kB 576kB/s\n",
            "Building wheels for collected packages: tqdm\n",
            "  Running setup.py bdist_wheel for tqdm ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-8kf29u8t/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n",
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gDvlo_jb_A9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import required modules**"
      ]
    },
    {
      "metadata": {
        "id": "6tvM_ph_8-Xl",
        "colab_type": "code",
        "outputId": "0bc0ead7-52bf-4b1b-f423-7461830cf1df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import datetime\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tflearn.data_utils import pad_sequences, to_categorical\n",
        "from tensorflow.contrib import slim\n",
        "from tqdm import tqdm, tnrange\n",
        "\n",
        "from tensorboardcolab import *"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4zlW6cONe9YE",
        "colab_type": "code",
        "outputId": "af88cb9e-7552-4a30-b5b6-17a13684c882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JZEIBoWN44Dv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Al3MGHfT46pq",
        "colab_type": "code",
        "outputId": "af2614fa-d2a5-4e86-b832-a5ff2b8cf9b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "df_real = pd.read_csv('headlines.csv', sep=',', usecols=['text', 'fake'])\n",
        "df_real = df_real.sample(frac=1)\n",
        "df_real = df_real[:50000]\n",
        "print(df_real.shape)\n",
        "df_real.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>774512</th>\n",
              "      <td>fire destroys truck in north qld</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515685</th>\n",
              "      <td>missing millionaire active on facebook</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58436</th>\n",
              "      <td>support for derby wharf upgrade</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169780</th>\n",
              "      <td>full text pm announces woods release</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955724</th>\n",
              "      <td>real madrid sacks manager ancelotti</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          text  fake\n",
              "774512        fire destroys truck in north qld     0\n",
              "515685  missing millionaire active on facebook     0\n",
              "58436          support for derby wharf upgrade     0\n",
              "169780    full text pm announces woods release     0\n",
              "955724     real madrid sacks manager ancelotti     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "P1HQ85uyARIJ",
        "colab_type": "code",
        "outputId": "b1ad439a-66be-4eaf-ef5d-350942227796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "cell_type": "code",
      "source": [
        "df_fake = pd.DataFrame(columns=['text', 'fake'])\n",
        "print(df_fake.shape)\n",
        "df_fake.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, fake]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "UFCT682iAiwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Test-, Trainingset and Hyper-Parameter"
      ]
    },
    {
      "metadata": {
        "id": "mat90D4fAnPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General Hyper-Parameter\n",
        "BATCH_SIZE = 32\n",
        "SEQ_LENGTH = 8 # average sequence length of the given sentences in the dataset\n",
        "TRAINING_SPLIT = 0.2\n",
        "TOTAL_EPOCH = 50#200\n",
        "\n",
        "# Discriminator Hyper-Parameter\n",
        "D_PRETRAIN_EPOCHS = 50\n",
        "D_EMB_SIZE = 100\n",
        "D_EMB_DIM = 64\n",
        "D_FILTER_SIZES = [2,3]\n",
        "D_NUM_CLASSES = 2\n",
        "D_NUM_FILTERS = 50\n",
        "\n",
        "# Generator Hyper-Parameter\n",
        "G_PRETRAIN_EPOCHS = 1000\n",
        "G_START_TOKEN = 0\n",
        "G_EMB_SIZE = 300\n",
        "G_EMB_DIM = 32\n",
        "G_HIDDEN_DIM = 32 # hidden state dimension of lstm cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dNP674HF22oU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_datasets(texts, labels):\n",
        "  tokenizer = init_tokenizer(texts)\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  text_seq = pad_sequences(sequences, maxlen=SEQ_LENGTH)\n",
        "    \n",
        "  labels = np.asarray(labels)\n",
        "\n",
        "  indices = np.arange(text_seq.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  text_seq = text_seq[indices]\n",
        "  labels = labels[indices]\n",
        "  test_size = int(TRAINING_SPLIT * text_seq.shape[0])\n",
        "  \n",
        "  X_train = text_seq[:test_size]\n",
        "  y_train = to_categorical(labels[:test_size], 2)\n",
        "  X_test = text_seq[test_size:]\n",
        "  y_test = labels[test_size:]\n",
        "  \n",
        "  return X_train, y_train, X_test, y_test\n",
        "\n",
        "def init_tokenizer(texts):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "  \n",
        "  return tokenizer\n",
        "\n",
        "def load_data(df):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  \n",
        "  for row in zip(df['text'], df['fake']):\n",
        "    texts.append(row[0].strip())\n",
        "    labels.append(row[1])\n",
        "  \n",
        "  return texts, labels\n",
        "\n",
        "def get_tokenizer():\n",
        "  texts, labels = load_data(df_real)\n",
        "  return init_tokenizer(texts)\n",
        "  \n",
        "def get_fake_data():\n",
        "  global df_fake\n",
        "  df_fake = df_fake.sample(frac=1)\n",
        "  \n",
        "  texts, labels = load_data(df_fake)\n",
        "  return get_datasets(texts, labels)\n",
        "\n",
        "def get_real_data():\n",
        "  global df_real\n",
        "  df_real = df_real.sample(frac=1)\n",
        "\n",
        "  texts, labels = load_data(df_real)\n",
        "  return get_datasets(texts, labels)\n",
        "\n",
        "def get_mixed_data():\n",
        "  # global notation ???\n",
        "  df = pd.concat([df_real, df_fake])\n",
        "  df = df.sample(frac=1)\n",
        "\n",
        "  texts, labels = load_data(df)\n",
        "  return get_datasets(texts, labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DxYDMwyi3u8H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer()\n",
        "\n",
        "WORD_INDEX = [word for idx, word in enumerate(tokenizer.word_index)]\n",
        "VOCAB_SIZE = len(WORD_INDEX) # number of different words in the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qIo4O_PW1Hfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discriminator\n",
        "model for classifying sequences (here headlines) as real or fake.\n",
        "In this implementation the discriminative model uses following layers: \n",
        "1.   embedding layer\n",
        "2.   convolution layer\n",
        "3.   max-pooling layer\n",
        "4.   softmax layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k3UdGGRc5FfF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator:\n",
        "  def __init__(self, batch_size, vocab_size, seq_length, emb_size, filter_sizes, num_classes, num_filters, learning_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.seq_length = seq_length\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length], name='d_X_input')\n",
        "    self.y_input = tf.placeholder(tf.int32, shape=[self.batch_size, self.num_classes], name='d_y_input')\n",
        "    self.dropout_keep_prob = tf.placeholder(tf.float32, name='d_dropout_keep_prob')\n",
        "    \n",
        "    # Keeping track of l2 regularization loss (optional)\n",
        "    self.l2_reg_lambda = 0.2\n",
        "    self.l2_loss = tf.constant(0.0)\n",
        "\n",
        "  def build_model(self):\n",
        "    with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):\n",
        "      self.embedding_layer = self.build_embedding_layer()\n",
        "      self.convolution_maxpool_layer = self.build_convolution_maxpool_layer()\n",
        "      self.scores, self.predictions = self.build_softmax_layer()\n",
        "\n",
        "      self.calc_mean_cross_entropy_loss()\n",
        "      self.calc_accuracy_and_cost()\n",
        "\n",
        "      self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
        "      \n",
        "      self.d_summary = tf.summary.merge([\n",
        "          tf.summary.scalar('d_loss', self.loss),\n",
        "          tf.summary.scalar('d_accuracy', self.accuracy)\n",
        "      ])\n",
        "        \n",
        "  def build_embedding_layer(self):\n",
        "    with tf.device('gpu:0'), tf.name_scope('d_embedding_layer'):\n",
        "      emb_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0))\n",
        "      emb_lookup = tf.nn.embedding_lookup(emb_matrix, self.X_input)\n",
        "      emb_lookup_expand = tf.expand_dims(emb_lookup, -1)\n",
        "      \n",
        "      return emb_lookup_expand\n",
        "    \n",
        "  def build_convolution_maxpool_layer(self):\n",
        "    with tf.name_scope('d_convolution_maxpool_layer'):\n",
        "      pooled_outputs = []\n",
        "      for filter_size in self.filter_sizes:\n",
        "        with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
        "          # Convolution Layer\n",
        "          filter_shape = [filter_size, self.emb_size, 1, self.num_filters]\n",
        "          W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='d_W')\n",
        "          b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name='d_b')\n",
        "          conv = tf.nn.conv2d(\n",
        "              input=self.embedding_layer,\n",
        "              filter=W,\n",
        "              strides=[1,1,1,1],\n",
        "              padding='VALID',\n",
        "              name='d_conv'\n",
        "          )\n",
        "          # Apply non-linearity - activation function\n",
        "          activation = tf.nn.relu(tf.nn.bias_add(conv, b), name='d_relu')\n",
        "          # Maxpooling over outputs\n",
        "          max_pooling = tf.nn.max_pool(\n",
        "              value=activation,\n",
        "              ksize=[1, self.seq_length-filter_size+1, 1, 1],\n",
        "              strides=[1,1,1,1],\n",
        "              padding='VALID',\n",
        "              name='max_pooling'\n",
        "          )\n",
        "          pooled_outputs.append(max_pooling)\n",
        "\n",
        "      # combine all the pooled features\n",
        "      self.num_filter_total = self.num_filters * len(self.filter_sizes)\n",
        "      h_pool = tf.concat(pooled_outputs, axis=3)\n",
        "      \n",
        "      return tf.reshape(h_pool, [-1, self.num_filter_total])\n",
        "        \n",
        "  def build_softmax_layer(self): \n",
        "    with tf.name_scope('highway'):\n",
        "      self.h_highway = self.highway(\n",
        "          self.convolution_maxpool_layer, self.convolution_maxpool_layer.get_shape()[1], 1, 0\n",
        "      )\n",
        "\n",
        "    with tf.name_scope('dropout'):\n",
        "      self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
        "      \n",
        "    with tf.name_scope('softmax_output'):\n",
        "      W_softmax = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              [self.num_filter_total, self.num_classes], \n",
        "              stddev=0.1\n",
        "          ), name='d_W_softmax'\n",
        "      )\n",
        "      b_softmax = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name='d_b_softmax')\n",
        "            \n",
        "      self.l2_loss += tf.nn.l2_loss(W_softmax)\n",
        "      self.l2_loss += tf.nn.l2_loss(b_softmax)\n",
        "      \n",
        "      self.scores = tf.nn.xw_plus_b(self.h_drop, W_softmax, b_softmax, name='d_scores')\n",
        "      #self.scores = tf.matmul(self.convolution_maxpool_layer, W_softmax) + b_softmax\n",
        "      self.truth_prob = tf.nn.softmax(self.scores, -1)[:, 1]\n",
        "      predictions = tf.argmax(self.scores, 1, name='d_predictions')\n",
        "      \n",
        "    return self.scores, predictions\n",
        "  \n",
        "  def calc_mean_cross_entropy_loss(self):\n",
        "    with tf.name_scope('mse_loss'):\n",
        "      losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=self.y_input)\n",
        "      self.loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
        "      \n",
        "  def calc_accuracy_and_cost(self):\n",
        "    with tf.name_scope('accuracy'):\n",
        "      correct_predictions = tf.equal(self.predictions, tf.argmax(self.y_input, 1))\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
        "      self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=self.y_input))\n",
        "      \n",
        "  def highway(self, input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n",
        "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
        "    t = sigmoid(Wy + b)\n",
        "    z = t * g(Wy + b) + (1 - t) * y\n",
        "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
        "    \"\"\"\n",
        "    with tf.variable_scope('highway'):\n",
        "      size = int(size)\n",
        "      for idx in range(num_layers):\n",
        "        g = f(slim.fully_connected(input_, size, scope='highway_lin_%d' % idx, activation_fn=None))\n",
        "        t = tf.sigmoid(slim.fully_connected(input_, size, scope='highway_gate_%d' % idx, activation_fn=None) + bias)\n",
        "\n",
        "        output = t * g + (1. - t) * input_\n",
        "        input_ = output\n",
        "        \n",
        "    return output\n",
        "  \n",
        "  def get_truth_prob(self, sess, X):\n",
        "    feed_dict = { self.X_input: X, self.dropout_keep_prob: 1.0 }\n",
        "        \n",
        "    return sess.run(self.truth_prob, feed_dict=feed_dict)\n",
        "        \n",
        "  def train(self, sess, X, y, dropout):\n",
        "    feed_dict = {\n",
        "        self.X_input: X,\n",
        "        self.y_input: y,\n",
        "        self.dropout_keep_prob: dropout\n",
        "    }\n",
        "    _, summary = sess.run([self.optimizer, self.d_summary], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B86yDcx3TC4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Discriminator model and train model"
      ]
    },
    {
      "metadata": {
        "id": "UnVPueM8TXwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "discriminator = Discriminator(BATCH_SIZE, VOCAB_SIZE, SEQ_LENGTH, D_EMB_SIZE, \n",
        "                              D_FILTER_SIZES, D_NUM_CLASSES, D_NUM_FILTERS, 0.001)\n",
        "discriminator.build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmjkZBeH557j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "\n",
        "LSTM with Reinforcement Learning for sequence generation."
      ]
    },
    {
      "metadata": {
        "id": "lcALithWy39s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib import rnn, layers, seq2seq, slim\n",
        "\n",
        "# inspired by https://www.oreilly.com/ideas/introduction-to-lstms-with-tensorflow\n",
        "# Decoder/Encoder approach from https://github.com/LA-JAMES/-Encoder-Decoder-Simple-Deep-LSTM-for-Tensorflow\n",
        "\n",
        "class Generator:\n",
        "  def __init__(self, batch_size, seq_length, vocab_size, emb_size, emb_dim, \n",
        "               hidden_dim, start_token, word_index, learning_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_length = seq_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.emb_dim = emb_dim\n",
        "    #self.hidden_dim = hidden_dim\n",
        "    #self.start_token = start_token\n",
        "    self.word_index = word_index\n",
        "    self.learning_rate = learning_rate\n",
        "    self.temperature = 1.0\n",
        "    self.grad_clip = 5.0\n",
        "    self.hidden_layer_sizes = [128]\n",
        "    self.hidden_layer_size = 128\n",
        "    \n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length], name='g_X_input')\n",
        "    self.y_input = tf.placeholder(tf.int32, shape=[None, self.vocab_size], name='g_y_input')\n",
        "    self.dropout_keep_prob = tf.placeholder(tf.float32, name='g_dropout_keep_prob')\n",
        "    self.rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.seq_length], name='rewards')\n",
        "    \n",
        "  def build_model(self):\n",
        "    with tf.variable_scope('generator', reuse=tf.AUTO_REUSE):\n",
        "      self.embedding_layer = self.build_embedding_layer(self.X_input)\n",
        "      \n",
        "      self.outputs = []\n",
        "      for i in range(self.seq_length+1):\n",
        "        output, final_state = self.build_lstm_layers()\n",
        "        self.outputs.append(output)\n",
        "                  \n",
        "      # PRETRAINING\n",
        "      \"\"\"self.pretrain_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "      self.pretrain_calc_loss()\n",
        "      self.pretrain_operation = slim.learning.create_train_op(\n",
        "          self.pretrain_loss, self.pretrain_optimizer, clip_gradient_norm=5.0\n",
        "      )\"\"\"\n",
        "      \n",
        "      # TRAINING\n",
        "      self.predictions, loss = self.get_prediction_and_loss()\n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "      self.train_operation = slim.learning.create_train_op(loss, optimizer, clip_gradient_norm=self.grad_clip)\n",
        "                  \n",
        "      # SUMMARY\n",
        "      \"\"\"self.g_pretrain_summary = tf.summary.scalar(\n",
        "          'g_pretrain_loss', pretrain_loss\n",
        "      )\"\"\"\n",
        "      self.g_train_summary = tf.summary.merge([\n",
        "          tf.summary.scalar('g_loss', loss),\n",
        "          tf.summary.scalar('g_reward', tf.reduce_mean(self.rewards))\n",
        "      ])\n",
        "        \n",
        "  def build_embedding_layer(self, X_input):\n",
        "    with tf.name_scope('g_embedding_layer'):\n",
        "      emb_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0))\n",
        "      emb_lookup = tf.nn.embedding_lookup(emb_matrix, X_input)\n",
        "      \n",
        "    return emb_lookup\n",
        "      \n",
        "  def build_lstm_layers(self):\n",
        "    with tf.name_scope('g_lstm_layers'):\n",
        "      layers = [rnn.LSTMCell(layer_size) for layer_size in self.hidden_layer_sizes]\n",
        "      dropouts = [rnn.DropoutWrapper(layer, output_keep_prob=self.dropout_keep_prob) for layer in layers]\n",
        "      cell = rnn.MultiRNNCell(dropouts) # , state_is_tuple=True)?\n",
        "      \n",
        "      initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
        "      outputs, final_state = tf.nn.dynamic_rnn(cell, self.embedding_layer, initial_state=initial_state)\n",
        "      \n",
        "    return outputs, final_state \n",
        "  \n",
        "  def get_prediction_and_loss(self):\n",
        "    predictions = []\n",
        "    W2 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[-1], self.vocab_size]), dtype=tf.float32)\n",
        "    b2 = tf.Variable(tf.zeros([1, self.vocab_size]), dtype=tf.float32)\n",
        "    for i in range(self.seq_length+1):\n",
        "      output = tf.reshape(self.outputs[i], [-1, self.hidden_layer_sizes[-1]])\n",
        "\n",
        "      logits = tf.matmul(output, W2) + b2 # broadcasted addition\n",
        "      #predictions.append(self.sample(tf.nn.softmax(logits), self.temperature))\n",
        "      predictions.append(tf.nn.softmax(logits))\n",
        "    \n",
        "    loss = -tf.reduce_sum(\n",
        "        tf.reduce_sum(\n",
        "            tf.one_hot(tf.to_int32(tf.reshape(self.X_input, [-1])), self.vocab_size, 1.0, 0.0) * \n",
        "            tf.log(tf.clip_by_value(tf.reshape(predictions[-1], [-1, self.vocab_size]), 1e-20, 1.0)),\n",
        "            1) * tf.reshape(self.rewards, [-1])\n",
        "    )\n",
        "    \n",
        "    return predictions, loss\n",
        "      \n",
        "  def generate(self, sess, given_tokens, dropout=0.75):\n",
        "    feed_dict = { self.X_input: given_tokens, self.dropout_keep_prob: dropout }\n",
        "    samples = sess.run([self.predictions[0]], feed_dict=feed_dict)\n",
        "    sentences, sequences = self.translate_samples(samples)\n",
        "    \n",
        "    return sentences, sequences\n",
        "    \n",
        "  def sample(self, predictions, temperature):\n",
        "    #preds = np.asarray(predictions[0]).astype('float64')\n",
        "    preds = np.log(predictions[0]) / temperature\n",
        "    preds = np.exp(preds) / np.sum(np.exp(preds))\n",
        "    rand = random.random() # range: [0,1]\n",
        "    total = 0.0\n",
        "    for i in range(len(preds)):\n",
        "      total += preds[i]\n",
        "      if total > rand:\n",
        "        return i\n",
        "      \n",
        "    return len(preds)-1\n",
        "    \n",
        "  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "  # REINFORCEMENT LEARNING\n",
        "  def rollout(self, sess, given_tokens, keep_steps=0, dropout=0.75):\n",
        "    feed_dict = { self.X_input: given_tokens, self.dropout_keep_prob: dropout }\n",
        "    output_tensors = self.predictions[keep_steps]\n",
        "      \n",
        "    return sess.run(output_tensors, feed_dict=feed_dict)\n",
        "  \n",
        "  def get_reward(self, sess, given_tokens, rollout_num, dis):\n",
        "    rewards = np.zeros((self.batch_size, self.seq_length))\n",
        "    for keep_num in range(1, self.seq_length):\n",
        "      for i in range(rollout_num):\n",
        "        # Markov Chain Monte Carlo Sample\n",
        "        mc_sample = self.rollout(sess, given_tokens, keep_steps=keep_num)\n",
        "        sentences, sequence = self.translate_samples(mc_sample)\n",
        "        rewards[:, keep_num] += dis.get_truth_prob(sess, sequence)\n",
        "            \n",
        "    rewards /= rollout_num\n",
        "    return rewards\n",
        "  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "  \n",
        "  def pretrain(self, sess, given_tokens, dropout=0.75):\n",
        "    feed_dict = { self.X_input: given_tokens, self.dropout_keep_prob: dropout }\n",
        "    sess.run(self.pretrain_operation, feed_dict=feed_dict)\n",
        "    _, summary = sess.run([self.pretrain_operation, self.g_pretrain_summary], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary\n",
        "  \n",
        "  def train(self, sess, given_tokens, rewards, dropout=0.75):\n",
        "    feed_dict = { self.X_input: given_tokens, self.rewards: rewards, self.dropout_keep_prob: dropout }\n",
        "    _, summary = sess.run([self.train_operation, self.g_train_summary], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary\n",
        "  \n",
        "  def translate_samples(self, sequence):\n",
        "    batch_softmax = np.reshape(sequence, [self.batch_size, self.seq_length, self.vocab_size])\n",
        "\n",
        "    sentences = []\n",
        "    vectors = []\n",
        "    for sequence in batch_softmax:\n",
        "      sentence = ''\n",
        "      vector = []\n",
        "      for pos in sequence:\n",
        "        vector_position = np.argmax(pos)\n",
        "        vector.append(vector_position)\n",
        "        word = self.word_index[vector_position]\n",
        "        sentence += word\n",
        "        sentence += ' '\n",
        "\n",
        "      sentences.append(sentence)\n",
        "      vectors.append(vector)\n",
        "\n",
        "    vectors = np.asarray(vectors)\n",
        "    return sentences, vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V03XEe_uVdwq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Generator model"
      ]
    },
    {
      "metadata": {
        "id": "PW7Hyz2J6jro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = Generator(BATCH_SIZE, SEQ_LENGTH, VOCAB_SIZE, G_EMB_SIZE, \n",
        "                      G_EMB_DIM, G_HIDDEN_DIM, G_START_TOKEN, WORD_INDEX, 0.0001)\n",
        "\n",
        "generator.build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uccb8c0sViK5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Start Adversarial Training"
      ]
    },
    {
      "metadata": {
        "id": "hN-drlXj1Kyj",
        "colab_type": "code",
        "outputId": "6a2d837c-09fe-44bb-bb3b-0c79b620ed60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "tbc=TensorBoardColab()\n",
        "\n",
        "LOG_DIR = '/tmp/log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://a2c472c6.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ymHD3nwRtS7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_fake_samples(headlines):\n",
        "  global df_fake\n",
        "  \n",
        "  for headline in headlines:\n",
        "    df_fake = df_fake.append({'text': headline , 'fake': 1} , ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GbdSazLhVllp",
        "colab_type": "code",
        "outputId": "baff91b2-31b5-4fda-9c16-7979a7fe019d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1391
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  print('start GAN training at', datetime.datetime.now())\n",
        "  writer = tbc.get_writer()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  \"\"\"# PRETRAINING GENERATOR\n",
        "  for epoch in range(G_PRETRAIN_EPOCHS):\n",
        "    # seq_length -1 und von 1 anfangen\n",
        "    X_train, y_train, X_test, y_test = get_mixed_data()\n",
        "    summary = generator.pretrain(sess, X_train[:BATCH_SIZE])\n",
        "    writer.add_summary(summary, epoch)\n",
        "    \n",
        "  # PRETRAIN DISCRIMINATOR\n",
        "  for eoch in range(D_PRETRAIN_EPOCHS):\n",
        "    fake_sentences, fake_sequences = generator.generate(sess, X_train[:BATCH_SIZE])\n",
        "    #translate_samples(tokenizer, fake_samples, df_fake)\n",
        "    X_train, y_train, X_test, y_test = get_data()\n",
        "    \n",
        "    for _ in range(3):\n",
        "      discriminator.train(sess, X_train, y_train, 5, BATCH_SIZE, .001)\"\"\"\n",
        "\n",
        "  # ADVERSARIAL TRAINING\n",
        "  for epoch in tnrange(TOTAL_EPOCH, desc='gan_epoch_loop'):\n",
        "    X_train, y_train, X_test, y_test = get_real_data()\n",
        "    # train generator for one step\n",
        "    for it in range(1):\n",
        "      fake_sentences, fake_sequences = generator.generate(sess, X_train[:BATCH_SIZE])\n",
        "      add_fake_samples(fake_sentences)\n",
        "      rewards = generator.get_reward(sess, fake_sequences, 16, discriminator)\n",
        "      summary = generator.train(sess, fake_sequences, rewards)\n",
        "    \n",
        "    writer.add_summary(summary, epoch)\n",
        "    \n",
        "    for _ in tnrange(5, desc='gen_train_loop'):\n",
        "      X_train, y_train, X_test, y_test = get_real_data()\n",
        "      samples, outputs = generator.generate(sess, X_train[:BATCH_SIZE])\n",
        "      add_fake_samples(samples)\n",
        "      \n",
        "      for _ in tnrange(3, desc='dis_train_loop'):\n",
        "        X_train, y_train, X_test, y_test = get_mixed_data()\n",
        "        summary = discriminator.train(sess, X_train[:BATCH_SIZE], y_train[:BATCH_SIZE], 0.4)\n",
        "        \n",
        "    print(df_fake.tail(2))\n",
        "    writer.add_summary(summary, epoch)\n",
        "    \n",
        "    \"\"\"tokenizer, word_index, X_real_train, y_real_train, X_real_test, y_real_test = get_real_data()\n",
        "    summary = sess.run(generator.image_summary, feed_dict={ generator.given_tokens: X_real_train[:BATCH_SIZE] })\n",
        "    writer.add_summary(summary, epoch)\"\"\"\n",
        "    \n",
        "  print('finish GAN training at', datetime.datetime.now())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start GAN training at 2019-01-10 10:12:24.357224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py:88: TqdmExperimentalWarning: Detect Google Colab 0.0.1a2 and thus load dummy ipywidgets package. Note that UI is different from that in Jupyter. See https://github.com/tqdm/tqdm/pull/640\n",
            "  \" See https://github.com/tqdm/tqdm/pull/640\".format(colab.__version__), TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gan_epoch_loop</span><progress style='margin:2px 4px;description_width:initial;' max='50' value='2'></progress>  4% 2/50 [01:31&lt;36:52, 46.08s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gen_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='5' value='5'></progress>100% 5/5 [00:40&lt;00:00,  8.05s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:06&lt;00:00,  2.12s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:05&lt;00:00,  1.98s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:05&lt;00:00,  1.96s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:05&lt;00:00,  1.95s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:05&lt;00:00,  2.00s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                                  text fake\n",
            "190  floral diversions bowraville nation 20yrs gumb...    1\n",
            "191  conscience performers ing 4yo oshane spagnuolo...    1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gen_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='5' value='5'></progress>100% 5/5 [00:39&lt;00:00,  7.98s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:06&lt;00:00,  2.01s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:06&lt;00:00,  2.04s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:05&lt;00:00,  1.94s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:05&lt;00:00,  1.93s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:05&lt;00:00,  2.01s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                                  text fake\n",
            "382  'three alimony mistrial fanned consoled deputy...    1\n",
            "383  limps mcewan cling tackles platypus essential'...    1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gen_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='5' value='2'></progress> 40% 2/5 [00:15&lt;00:23,  7.93s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:05&lt;00:00,  1.92s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:06&lt;00:00,  2.03s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='1'></progress> 33% 1/3 [00:01&lt;00:03,  1.92s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9e71cbf78a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtnrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dis_train_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mixed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a465310c50fc>\u001b[0m in \u001b[0;36mget_mixed_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-a465310c50fc>\u001b[0m in \u001b[0;36mget_datasets\u001b[0;34m(texts, labels)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a465310c50fc>\u001b[0m in \u001b[0;36minit_tokenizer\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    220\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtranslate_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtranslate_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2TgllHrMIdaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "print(df_fake.tail())\n",
        "\n",
        "count = 1\n",
        "saved = False\n",
        "while saved != True:\n",
        "  filename = 'generated_headlines_%d.csv' % count\n",
        "  filepath = Path('./results/%s' % filename)\n",
        "  \n",
        "  if filepath.exists():\n",
        "    count = count+1\n",
        "  else:\n",
        "    df_fake.to_csv(filepath, sep='\\t', encoding='utf-8')\n",
        "    saved = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qaXiLTyPg_AM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}