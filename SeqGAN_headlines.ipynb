{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqGAN_headlines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogch/masterproject_gan/blob/master/SeqGAN_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sjo87PQzZyc4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Generation using GAN\n",
        "\n",
        "This notebook generates news headlines using the Machine Learning technology GAN (Generative Adversarial Networks)."
      ]
    },
    {
      "metadata": {
        "id": "OhjZVsMcaMqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yo4no6KeYnM3",
        "colab_type": "code",
        "outputId": "c8c14500-a402-41da-a5f6-bc7129aad895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnmI7T2sZAkv",
        "colab_type": "code",
        "outputId": "f742bbf7-0ebb-4e1d-a939-55bac1bdad83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Masterproject\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Masterproject\n",
            "abcnews-date-text.csv\t screenshots\n",
            "headlines.csv\t\t SeqGAN\n",
            "headlines-short.csv\t SeqGAN_headlines_dataloading_experiments.ipynb\n",
            "news-headlines.db\t SeqGAN_headlines.ipynb\n",
            "news-headlines-short.db\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vRnegJVw-2YX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install required dependencies manually**"
      ]
    },
    {
      "metadata": {
        "id": "bS58gcTIfiCx",
        "colab_type": "code",
        "outputId": "6bcad723-23c2-476e-91e5-7e02b08cf5ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tflearn\n",
        "!pip install tqdm\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (4.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.14.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->tflearn) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     | 481kB 1.3MB/s\n",
            "Building wheels for collected packages: tqdm\n",
            "  Running setup.py bdist_wheel for tqdm ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-6zb6xju2/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gDvlo_jb_A9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import required modules**"
      ]
    },
    {
      "metadata": {
        "id": "6tvM_ph_8-Xl",
        "colab_type": "code",
        "outputId": "c76f0096-9a57-478c-ea1d-37c3b8b53a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "import datetime\n",
        "\n",
        "from tflearn.datasets import imdb\n",
        "from tflearn.data_utils import pad_sequences, to_categorical\n",
        "\n",
        "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tqdm import tqdm, tnrange"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4zlW6cONe9YE",
        "colab_type": "code",
        "outputId": "ba244285-f476-4273-dfac-83d4a261344f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JZEIBoWN44Dv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Al3MGHfT46pq",
        "colab_type": "code",
        "outputId": "de7f27f3-52ce-41a7-f951-fc154f145174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('headlines.csv', sep=',', index_col='id')\n",
        "df = df.sample(frac=1)\n",
        "# 200000 samples needs 17-23 minutes per epoch\n",
        "# 100000 samples needs 5 minutes per epoch\n",
        "df = df[:50000]\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(50000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>291321</th>\n",
              "      <td>20070313</td>\n",
              "      <td>academic warns of bowen basin mining book nega...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373339</th>\n",
              "      <td>20080329</td>\n",
              "      <td>united rivals skating on thin ice</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1077232</th>\n",
              "      <td>20170519</td>\n",
              "      <td>senior bureaucrats knew of oakden problems</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302973</th>\n",
              "      <td>20070508</td>\n",
              "      <td>govt acknowledges threat of climate change with</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1088895</th>\n",
              "      <td>20170825</td>\n",
              "      <td>burnie museum advocate collection</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         publish_date                                               text  fake\n",
              "id                                                                            \n",
              "291321       20070313  academic warns of bowen basin mining book nega...     0\n",
              "373339       20080329                  united rivals skating on thin ice     0\n",
              "1077232      20170519         senior bureaucrats knew of oakden problems     0\n",
              "302973       20070508    govt acknowledges threat of climate change with     0\n",
              "1088895      20170825                  burnie museum advocate collection     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "UFCT682iAiwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Test-, Trainingset and Hyper-Parameter"
      ]
    },
    {
      "metadata": {
        "id": "mat90D4fAnPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General Hyper-Parameter\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = 5000 # 20\n",
        "SEQ_LENGTH = 100\n",
        "TRAINING_SPLIT = 0.2\n",
        "TOTAL_BATCH = 200\n",
        "\n",
        "# Discriminator Hyper-Parameter\n",
        "D_EMB_SIZE = 100\n",
        "D_EMB_DIM = 64 # embedding dimension\n",
        "D_FILTER_SIZES = [2,3]\n",
        "D_NUM_CLASSES = 2\n",
        "D_NUM_FILTERS = 50\n",
        "\n",
        "# Generator Hyper-Parameter\n",
        "G_START_TOKEN = 0\n",
        "G_EMB_SIZE = 100\n",
        "G_EMB_DIM = 32\n",
        "G_HIDDEN_DIM = 32 # hidden state dimension of lstm cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dNP674HF22oU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_datasets(texts, labels, tokenizer=None):\n",
        "  if tokenizer is None:\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  word_index = tokenizer.word_index\n",
        "  text_data = pad_sequences(sequences, maxlen=SEQ_LENGTH)\n",
        "\n",
        "  labels = np.asarray(labels)\n",
        "\n",
        "  indices = np.arange(text_data.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  text_data = text_data[indices]\n",
        "  labels = labels[indices]\n",
        "  test_size = int(TRAINING_SPLIT * text_data.shape[0])\n",
        "  \n",
        "  X_train = text_data[:test_size]\n",
        "  y_train = to_categorical(labels[:test_size], 2)\n",
        "  X_test = text_data[test_size:]\n",
        "  y_test = labels[test_size:]\n",
        "\n",
        "  return tokenizer, word_index, X_train, y_train, X_test, y_test\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for row in zip(df['text'], df['fake']):\n",
        "  texts.append(row[0].strip())\n",
        "  labels.append(row[1])\n",
        "  \n",
        "tokenizer, word_index, X_train, y_train, X_test, y_test = get_datasets(texts, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qIo4O_PW1Hfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discriminator\n",
        "model for classifying sequences (here headlines) as real or fake.\n",
        "In this implementation the discriminative model uses following layers: \n",
        "1.   embedding layer\n",
        "2.   convolution layer\n",
        "3.   max-pooling layer\n",
        "4.   softmax layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k3UdGGRc5FfF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "class Discriminator:\n",
        "  def __init__(self, vocab_size, seq_length, emb_size, filter_sizes, num_classes, num_filters):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.seq_length = seq_length\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "\n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[None, self.seq_length], name='X_input')\n",
        "    self.y_input = tf.placeholder(tf.float32, shape=[None, self.num_classes], name='y_input')\n",
        "    self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
        "    \n",
        "    # Keeping track of l2 regularization loss (optional)\n",
        "    self.l2_reg_lambda = 0.0\n",
        "    self.l2_loss = tf.constant(0.0)\n",
        "\n",
        "  def build_model(self):\n",
        "    self.embedding_layer = self.build_embedding_layer()\n",
        "    self.convolution_maxpool_layer = self.build_convolution_maxpool_layer()\n",
        "    self.scores, self.predictions = self.build_softmax_layer()\n",
        "    \n",
        "    self.calc_mean_cross_entropy_loss()\n",
        "    self.calc_accuracy_and_cost()\n",
        "        \n",
        "  def build_embedding_layer(self):\n",
        "    with tf.device('gpu:0'), tf.name_scope('embedding_layer'):\n",
        "      W_emb = tf.Variable(\n",
        "          initial_value=tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0), \n",
        "          name='W'\n",
        "      )\n",
        "      emb_chars = tf.nn.embedding_lookup(W_emb, self.X_input)\n",
        "      self.emb_chars_expand = tf.expand_dims(emb_chars, -1)\n",
        "    \n",
        "  def build_convolution_maxpool_layer(self):\n",
        "    pooled_outputs = []\n",
        "    for filter_size in self.filter_sizes:\n",
        "      with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
        "        # Convolution Layer\n",
        "        filter_shape = [filter_size, self.emb_size, 1, self.num_filters]\n",
        "        W_filters = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
        "        b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name='b')\n",
        "        conv = tf.nn.conv2d(\n",
        "            input=self.emb_chars_expand,\n",
        "            filter=W_filters,\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='conv'\n",
        "        )\n",
        "        # Apply non-linearity - activation function\n",
        "        activation = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
        "        # Maxpooling over outputs\n",
        "        max_pooling = tf.nn.max_pool(\n",
        "            value=activation,\n",
        "            ksize=[1, self.seq_length-filter_size+1, 1, 1],\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='max_pooling'\n",
        "        )\n",
        "        pooled_outputs.append(max_pooling)\n",
        "              \n",
        "    self.num_filter_total = self.num_filters * len(self.filter_sizes)\n",
        "    h_pool = tf.concat(pooled_outputs, axis=3)\n",
        "    return tf.reshape(h_pool, [-1, self.num_filter_total])\n",
        "        \n",
        "  def build_softmax_layer(self): \n",
        "    with tf.name_scope('highway'):\n",
        "      self.h_highway = self.highway(\n",
        "          self.convolution_maxpool_layer, self.convolution_maxpool_layer.get_shape()[1], 1, 0\n",
        "      )\n",
        "\n",
        "    with tf.name_scope('dropout'):\n",
        "      self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
        "      \n",
        "    with tf.name_scope('softmax_output'):\n",
        "      W_softmax = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              [self.num_filter_total, self.num_classes], \n",
        "              stddev=0.1\n",
        "          ), name='W_softmax'\n",
        "      )\n",
        "      b_softmax = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name='b_softmax')\n",
        "            \n",
        "      self.l2_loss += tf.nn.l2_loss(W_softmax)\n",
        "      self.l2_loss += tf.nn.l2_loss(b_softmax)\n",
        "      \n",
        "      #self.scores = tf.nn.xw_plus_b(self.h_drop, W_softmax, b_softmax, name='scores')\n",
        "      self.scores = tf.matmul(self.convolution_maxpool_layer, W_softmax) + b_softmax\n",
        "      self.ypred_for_auc = tf.nn.softmax(self.scores)\n",
        "      predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "      \n",
        "    return self.scores, predictions\n",
        "  \n",
        "  def calc_mean_cross_entropy_loss(self):\n",
        "    with tf.name_scope('cross_entropy_loss'):\n",
        "      losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.y_input)\n",
        "      self.loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
        "      \n",
        "  def calc_accuracy_and_cost(self):\n",
        "    with tf.name_scope('accuracy'):\n",
        "      correct_predictions = tf.equal(self.predictions, tf.argmax(self.y_input, 1))\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
        "      self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.y_input))\n",
        "      \n",
        "  def highway(self, input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n",
        "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
        "    t = sigmoid(Wy + b)\n",
        "    z = t * g(Wy + b) + (1 - t) * y\n",
        "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
        "    \"\"\"\n",
        "    with tf.variable_scope('highway'):\n",
        "      size = int(size)\n",
        "      for idx in range(num_layers):\n",
        "        g = f(slim.fully_connected(input_, size, scope='highway_lin_%d' % idx, activation_fn=None))\n",
        "        t = tf.sigmoid(slim.fully_connected(input_, size, scope='highway_gate_%d' % idx, activation_fn=None) + bias)\n",
        "\n",
        "        output = t * g + (1. - t) * input_\n",
        "        input_ = output\n",
        "        \n",
        "    return output\n",
        "        \n",
        "  def train(self, X, y, num_epochs, batch_size, learning_rate):\n",
        "    with tf.name_scope('loss'):\n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.initialize_all_variables())\n",
        "      epoch_i = 0\n",
        "      \n",
        "      while epoch_i < num_epochs:\n",
        "        batch_i = 0\n",
        "        batch_losses = []\n",
        "        batch_accs = []\n",
        "            \n",
        "        \n",
        "        for i in tnrange(batch_size, X.shape[0], batch_size):\n",
        "          X_batch, y_batch = X[batch_i:i], y[batch_i:i]\n",
        "          feed_dict = {\n",
        "              self.X_input: X_batch,\n",
        "              self.y_input: y_batch,\n",
        "              self.dropout_keep_prob: 0.75\n",
        "          }\n",
        "          sess.run(optimizer, feed_dict)\n",
        "          loss, accuracy = sess.run([self.cost, self.accuracy], feed_dict)\n",
        "          \n",
        "          batch_accs.append(accuracy)\n",
        "          batch_losses.append(loss)\n",
        "    \n",
        "        time_str = datetime.datetime.now().isoformat()\n",
        "        print(\"{}: epoch: {}, loss: {}, acc: {}\".format(time_str, epoch_i, np.mean(batch_losses), np.mean(batch_accs)))\n",
        "        epoch_i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B86yDcx3TC4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Discriminator model and train model"
      ]
    },
    {
      "metadata": {
        "id": "UnVPueM8TXwc",
        "colab_type": "code",
        "outputId": "8b4e2144-8808-4146-fb92-f08ee5ed4556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator(VOCAB_SIZE, SEQ_LENGTH, D_EMB_SIZE,\n",
        "                              D_FILTER_SIZES, D_NUM_CLASSES, D_NUM_FILTERS)\n",
        "discriminator.build_model()\n",
        "\n",
        "#discriminator.train(X_train, y_train, 5, BATCH_SIZE, .001)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-cfca5b043ebd>:98: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hmjkZBeH557j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "\n",
        "simple LSTM network for sequence generation."
      ]
    },
    {
      "metadata": {
        "id": "4KvsR4J66X2i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator:\n",
        "  def __init__(self, batch_size, seq_length, vocab_size, emb_size, emb_dim, \n",
        "               hidden_dim, start_token, learning_rate, reward_gamma):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_length = seq_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.start_token = tf.constant([start_token] * self.batch_size, dtype=tf.int32)\n",
        "    self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
        "    self.reward_gamma = reward_gamma\n",
        "    self.g_params = []\n",
        "    self.d_params = []\n",
        "    #self.temperature = 1.0\n",
        "    self.grad_clip = 5.0\n",
        "    \n",
        "    self.expected_reward = tf.Variable(tf.zeros([self.seq_length]))\n",
        "    \n",
        "    self.h0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
        "    self.h0 = tf.stack([self.h0, self.h0])\n",
        "    \n",
        "    with tf.variable_scope('generator'):\n",
        "      self.g_embeddings = tf.Variable(tf.random_normal([self.emb_size, self.emb_dim], stddev=0.1)) # init embeddings matrix\n",
        "      self.g_params.append(self.g_embeddings)\n",
        "      self.g_recurrent_unit = self.create_recurrent_unit(self.g_params) # maps h_tm1 to h_1 for generator\n",
        "      self.g_output_unit = self.create_output_unit(self.g_params) # maps h_t to o_t (output token logits)\n",
        "      \n",
        "    # sequence of tokens generated by the generator\n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length])\n",
        "    # get from rollout policy and discriminator\n",
        "    self.rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.seq_length])\n",
        "    \n",
        "    with tf.device('gpu:0'):\n",
        "      # seq_length * batch_size\n",
        "      self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.X_input), perm=[1,0,2])\n",
        "      \n",
        "    self.gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
        "    self.gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
        "    \n",
        "    def g_recursion(i, x_t, h_tm1, gen_o, gen_x):\n",
        "      h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "      o_t = self.g_output_unit(h_t)\n",
        "      log_prob = tf.log(tf.nn.softmax(o_t))\n",
        "      next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
        "      x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)\n",
        "      gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(\n",
        "          tf.one_hot(next_token, self.emb_size, 1.0, 0.0), tf.nn.softmax(o_t)), 1))\n",
        "      gen_x = gen_x.write(i, next_token)\n",
        "      \n",
        "      return i+1, x_tp1, h_t, gen_o, gen_x\n",
        "    \n",
        "    _, _, _, self.gen_o, self.gen_x = control_flow_ops.while_loop(\n",
        "        cond=lambda i, _1, _2, _3, _4: i < self.seq_length, \n",
        "        body=g_recursion, \n",
        "        loop_vars=(\n",
        "            tf.constant(0, dtype=tf.int32), \n",
        "            tf.nn.embedding_lookup(self.g_embeddings, self.start_token), \n",
        "            self.h0, self.gen_o, self.gen_x\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    self.gen_x = self.gen_x.stack() # seq_length * batch_size\n",
        "    self.gen_x = tf.transpose(self.gen_x, perm=[1,0]) # batch_size * seq_length\n",
        "    \n",
        "    # Supervised pretraining for generator\n",
        "    #----------------------------------------\n",
        "    g_predictions = tensor_array_ops.TensorArray(\n",
        "        dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True\n",
        "    )\n",
        "    \n",
        "    ta_emb_x = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length)\n",
        "    ta_emb_x = ta_emb_x.unstack(self.processed_x)\n",
        "    \n",
        "    def pretrain_recursion(i, x_t, h_tm1, g_predictions):\n",
        "      h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "      o_t = self.g_output_unit(h_t)\n",
        "      g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))\n",
        "      x_tp1 = ta_emb_x.read(i)\n",
        "      \n",
        "      return i+1, x_tp1, h_t, g_predictions\n",
        "    \n",
        "    _, _, _, self.g_predictions = control_flow_ops.while_loop(\n",
        "        cond=lambda i, _1, _2, _3: i < self.seq_length,\n",
        "        body=pretrain_recursion,\n",
        "        loop_vars=(\n",
        "            tf.constant(0, dtype=tf.int32),\n",
        "            tf.nn.embedding_lookup(self.g_embeddings, self.start_token),\n",
        "            self.h0, g_predictions\n",
        "        )\n",
        "    )\n",
        "\n",
        "    self.g_predictions = tf.transpose(self.g_predictions.stack(), perm=[1,0,2])\n",
        "    \n",
        "    # loss of pretraining\n",
        "    self.pretrain_loss = -tf.reduce_sum(\n",
        "        tf.one_hot(tf.to_int32(tf.reshape(self.X_input, [-1])), self.emb_size, 1.0, 0.0) * tf.log(\n",
        "            tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.emb_size]), 1e-20, 1.0)\n",
        "        )\n",
        "    ) / (self.seq_length * self.batch_size)\n",
        "    \n",
        "    # updates from training\n",
        "    pretrain_opt = self.g_optimizer(self.learning_rate)\n",
        "    \n",
        "    self.pretrain_grad, _ = tf.clip_by_global_norm(tf.gradients(self.pretrain_loss, self.g_params), self.grad_clip)\n",
        "    self.pretrain_updates = pretrain_opt.apply_gradients(zip(self.pretrain_grad, self.g_params))\n",
        "    \n",
        "    # Reinforcement / Unsupervised training\n",
        "    #-----------------------------------------\n",
        "    self.g_loss = -tf.reduce_sum(\n",
        "        tf.reduce_sum(\n",
        "            tf.one_hot(tf.to_int32(tf.reshape(self.X_input, [-1])), self.emb_size, 1.0, 0.0) * tf.log(\n",
        "                tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.emb_size]), 1e-20, 1.0)\n",
        "            ), 1\n",
        "        ) * tf.reshape(self.rewards, [-1])\n",
        "    )\n",
        "    \n",
        "    g_opt = self.g_optimizer(self.learning_rate)\n",
        "    \n",
        "    self.g_grad, _ = tf.clip_by_global_norm(tf.gradients(self.g_loss, self.g_params), self.grad_clip)\n",
        "    self.g_updates = g_opt.apply_gradients(zip(self.g_grad, self.g_params))\n",
        "    \n",
        "  def g_optimizer(self, *args, **kwargs):\n",
        "    return tf.train.AdamOptimizer(*args, **kwargs)\n",
        "    \n",
        "  def generate(self, sess):\n",
        "    outputs = sess.run(self.gen_x)\n",
        "    return outputs\n",
        "  \n",
        "  def pretrain_step(self, sess, x):\n",
        "    outputs = sess.run([self.pretrain_updates, self.pretrain_loss], feed_dict={self.X_input: x})\n",
        "    return outputs\n",
        "  \n",
        "  def create_recurrent_unit(self, params):\n",
        "    # weights and bias for input and hidden tensor\n",
        "    self.Wi = tf.Variable(tf.random_normal([self.emb_dim, self.hidden_dim]))\n",
        "    self.Ui = tf.Variable(tf.random_normal([self.hidden_dim, self.hidden_dim]))\n",
        "    self.bi = tf.Variable(tf.random_normal([self.hidden_dim]))\n",
        "\n",
        "    self.Wf = tf.Variable(tf.random_normal([self.emb_dim, self.hidden_dim]))\n",
        "    self.Uf = tf.Variable(tf.random_normal([self.hidden_dim, self.hidden_dim]))\n",
        "    self.bf = tf.Variable(tf.random_normal([self.hidden_dim]))\n",
        "\n",
        "    self.Wog = tf.Variable(tf.random_normal([self.emb_dim, self.hidden_dim]))\n",
        "    self.Uog = tf.Variable(tf.random_normal([self.hidden_dim, self.hidden_dim]))\n",
        "    self.bog = tf.Variable(tf.random_normal([self.hidden_dim]))\n",
        "\n",
        "    self.Wc = tf.Variable(tf.random_normal([self.emb_dim, self.hidden_dim]))\n",
        "    self.Uc = tf.Variable(tf.random_normal([self.hidden_dim, self.hidden_dim]))\n",
        "    self.bc = tf.Variable(tf.random_normal([self.hidden_dim]))\n",
        "\n",
        "    params.extend([self.Wi, self.Ui, self.bi, self.Wf, self.Uf, self.bf, \n",
        "                   self.Wog, self.Uog, self.bog, self.Wc, self.Uc, self.bc])\n",
        "\n",
        "    def unit(x, hidden_memory):\n",
        "      prev_hidden_state, c_prev = tf.unstack(hidden_memory)\n",
        "\n",
        "      # Input Gate\n",
        "      i = tf.sigmoid(tf.matmul(x, self.Wi) + tf.matmul(prev_hidden_state, self.Ui) + self.bi)\n",
        "      # Forget Gate\n",
        "      f = tf.sigmoid(tf.matmul(x, self.Wf) + tf.matmul(prev_hidden_state, self.Uf) + self.bf)\n",
        "      # Output Gate\n",
        "      o = tf.sigmoid(tf.matmul(x, self.Wog) + tf.matmul(prev_hidden_state, self.Uog) + self.bog)\n",
        "\n",
        "      # New Memory Cell\n",
        "      c_ = tf.nn.tanh(tf.matmul(x, self.Wc) + tf.matmul(prev_hidden_state, self.Uc) + self.bc)\n",
        "      # Final Memory Cell\n",
        "      c = f * c_prev + i * c_\n",
        "\n",
        "      # Current Hidden State\n",
        "      curr_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "      return tf.stack([curr_hidden_state, c])\n",
        "\n",
        "    return unit\n",
        "\n",
        "  def create_output_unit(self, params):\n",
        "    self.Wo = tf.Variable(tf.random_normal([self.hidden_dim, self.emb_size]))\n",
        "    self.bo = tf.Variable(tf.random_normal([self.emb_size]))\n",
        "    params.extend([self.Wo, self.bo])\n",
        "\n",
        "    def unit(hidden_memory_tuple):\n",
        "      hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "      logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "\n",
        "      return logits\n",
        "\n",
        "    return unit\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nNXWnagxP-8Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Rollout:\n",
        "  def __init__(self, lstm, update_rate):\n",
        "    self.lstm = lstm\n",
        "    self.update_rate = update_rate\n",
        "    \n",
        "    self.batch_size = self.lstm.batch_size\n",
        "    self.seq_length = self.lstm.seq_length\n",
        "    self.emb_size = self.lstm.emb_size\n",
        "    self.emb_dim = self.lstm.emb_dim\n",
        "    self.hidden_dim = self.lstm.hidden_dim\n",
        "    self.start_token = tf.identity(self.lstm.start_token)\n",
        "    self.learning_rate = self.lstm.learning_rate\n",
        "    \n",
        "    self.g_embeddings = tf.identity(self.lstm.g_embeddings)\n",
        "    self.g_recurrent_unit = self.create_recurrent_unit()\n",
        "    self.g_output_unit = self.create_output_unit()\n",
        "    \n",
        "    # sequence of tokens generated by the generator\n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length])\n",
        "    self.given_num = tf.placeholder(tf.int32)\n",
        "    \n",
        "    with tf.device('gpu:0'):\n",
        "      # seq_length * batch_size\n",
        "      self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.X_input), perm=[1,0,2])\n",
        "      \n",
        "    ta_emb_x = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length)\n",
        "    ta_emb_x = ta_emb_x.unstack(self.processed_x)\n",
        "    \n",
        "    ta_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.seq_length)\n",
        "    ta_x = ta_x.unstack(tf.transpose(self.X_input, perm=[1,0]))\n",
        "    \n",
        "    self.h0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
        "    self.h0 = tf.stack([self.h0, self.h0])\n",
        "    \n",
        "    gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
        "    \n",
        "    # When current index i < given_num, use the provided tokens as the input at each time step\n",
        "    def g_recursion_1(i, x_t, h_tm1, given_num, gen_x):\n",
        "      h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "      x_tp1 = ta_emb_x.read(i)\n",
        "      gen_x = gen_x.write(i, ta_x.read(i))\n",
        "      \n",
        "      return i+1, x_tp1, h_t, given_num, gen_x\n",
        "    \n",
        "    # When current index i >= given_num, start roll-out, use the output as time step t as the input at time step t+1\n",
        "    def g_recursion_2(i, x_t, h_tm1, given_num, gen_x):\n",
        "      h_t = self.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
        "      o_t = self.g_output_unit(h_t)  # batch x vocab , logits not prob\n",
        "      log_prob = tf.log(tf.nn.softmax(o_t))\n",
        "      next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
        "      x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
        "      gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
        "      return i + 1, x_tp1, h_t, given_num, gen_x\n",
        "    \n",
        "    i, x_t, h_tm1, given_num, self.gen_x = control_flow_ops.while_loop(\n",
        "        cond=lambda i, _1, _2, given_num, _4: i < given_num,\n",
        "        body=g_recursion_1,\n",
        "        loop_vars=(\n",
        "            tf.constant(0, dtype=tf.int32),\n",
        "            tf.nn.embedding_lookup(self.g_embeddings, self.start_token), \n",
        "            self.h0, self.given_num, gen_x\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    _, _, _, _, self.gen_x = control_flow_ops.while_loop(\n",
        "        cond=lambda i, _1, _2, _3, _4: i < self.seq_length, \n",
        "        body=g_recursion_2, \n",
        "        loop_vars=(i, x_t, h_tm1, given_num, self.gen_x)\n",
        "    )\n",
        "    \n",
        "    self.gen_x = self.gen_x.stack() # seq_length * batch_size\n",
        "    self.gen_x = tf.transpose(self.gen_x, perm=[1,0]) # batch_size * seq_length\n",
        "    \n",
        "  def get_reward(self, sess, X_input, rollout_num, dis):\n",
        "    rewards = []\n",
        "    for i in tnrange(rollout_num):\n",
        "      # given num between 1 and seq_length - 1 for a part complete setence\n",
        "      for given_num in tnrange(1, self.seq_length):\n",
        "        feed = {self.X_input: X_input, self.given_num: given_num}\n",
        "        samples = sess.run(self.gen_x, feed)\n",
        "        feed = {dis.X_input: samples, dis.dropout_keep_prob: 1.0}\n",
        "        ypred_for_auc = sess.run(dis.ypred_for_auc, feed)\n",
        "        ypred = np.array([item[1] for item in ypred_for_auc])\n",
        "        if i == 0:\n",
        "          rewards.append(ypred)\n",
        "        else:\n",
        "          rewards[given_num-1] += ypred\n",
        "          \n",
        "      # last token reward\n",
        "      feed = {dis.X_input: X_input, dis.dropout_keep_prob: 1.0}\n",
        "      ypred_for_auc = sess.run(dis.ypred_for_auc, feed)\n",
        "      ypred = np.array([item[1] for item in ypred_for_auc])\n",
        "      if i == 0:\n",
        "        rewards.append(ypred)\n",
        "      else:\n",
        "        # complete sentence reward\n",
        "        rewards[self.seq_length-1] += ypred\n",
        "        \n",
        "    rewards = np.transpose(np.array(rewards)) / (1.0 * rollout_num)\n",
        "    return rewards\n",
        "  \n",
        "  def create_recurrent_unit(self):\n",
        "    # weights and bias for input and hidden tensor\n",
        "    self.Wi = tf.identity(self.lstm.Wi)\n",
        "    self.Ui = tf.identity(self.lstm.Ui)\n",
        "    self.bi = tf.identity(self.lstm.bi)\n",
        "\n",
        "    self.Wf = tf.identity(self.lstm.Wf)\n",
        "    self.Uf = tf.identity(self.lstm.Uf)\n",
        "    self.bf = tf.identity(self.lstm.bf)\n",
        "\n",
        "    self.Wog = tf.identity(self.lstm.Wog)\n",
        "    self.Uog = tf.identity(self.lstm.Uog)\n",
        "    self.bog = tf.identity(self.lstm.bog)\n",
        "\n",
        "    self.Wc = tf.identity(self.lstm.Wc)\n",
        "    self.Uc = tf.identity(self.lstm.Uc)\n",
        "    self.bc = tf.identity(self.lstm.bc)\n",
        "\n",
        "    def unit(x, hidden_memory):\n",
        "      prev_hidden_state, c_prev = tf.unstack(hidden_memory)\n",
        "\n",
        "      # Input Gate\n",
        "      i = tf.sigmoid(tf.matmul(x, self.Wi) + tf.matmul(prev_hidden_state, self.Ui) + self.bi)\n",
        "      # Forget Gate\n",
        "      f = tf.sigmoid(tf.matmul(x, self.Wf) + tf.matmul(prev_hidden_state, self.Uf) + self.bf)\n",
        "      # Output Gate\n",
        "      o = tf.sigmoid(tf.matmul(x, self.Wog) + tf.matmul(prev_hidden_state, self.Uog) + self.bog)\n",
        "\n",
        "      # New Memory Cell\n",
        "      c_ = tf.nn.tanh(tf.matmul(x, self.Wc) + tf.matmul(prev_hidden_state, self.Uc) + self.bc)\n",
        "      # Final Memory Cell\n",
        "      c = f * c_prev + i * c_\n",
        "\n",
        "      # Current Hidden State\n",
        "      curr_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "      return tf.stack([curr_hidden_state, c])\n",
        "\n",
        "    return unit\n",
        "  \n",
        "    def update_recurrent_unit(self):\n",
        "      # Weights and Bias for input and hidden tensor\n",
        "      self.Wi = self.update_rate * self.Wi + (1 - self.update_rate) * tf.identity(self.lstm.Wi)\n",
        "      self.Ui = self.update_rate * self.Ui + (1 - self.update_rate) * tf.identity(self.lstm.Ui)\n",
        "      self.bi = self.update_rate * self.bi + (1 - self.update_rate) * tf.identity(self.lstm.bi)\n",
        "\n",
        "      self.Wf = self.update_rate * self.Wf + (1 - self.update_rate) * tf.identity(self.lstm.Wf)\n",
        "      self.Uf = self.update_rate * self.Uf + (1 - self.update_rate) * tf.identity(self.lstm.Uf)\n",
        "      self.bf = self.update_rate * self.bf + (1 - self.update_rate) * tf.identity(self.lstm.bf)\n",
        "\n",
        "      self.Wog = self.update_rate * self.Wog + (1 - self.update_rate) * tf.identity(self.lstm.Wog)\n",
        "      self.Uog = self.update_rate * self.Uog + (1 - self.update_rate) * tf.identity(self.lstm.Uog)\n",
        "      self.bog = self.update_rate * self.bog + (1 - self.update_rate) * tf.identity(self.lstm.bog)\n",
        "\n",
        "      self.Wc = self.update_rate * self.Wc + (1 - self.update_rate) * tf.identity(self.lstm.Wc)\n",
        "      self.Uc = self.update_rate * self.Uc + (1 - self.update_rate) * tf.identity(self.lstm.Uc)\n",
        "      self.bc = self.update_rate * self.bc + (1 - self.update_rate) * tf.identity(self.lstm.bc)\n",
        "\n",
        "      def unit(x, hidden_memory_tm1):\n",
        "        previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
        "\n",
        "        # Input Gate\n",
        "        i = tf.sigmoid(tf.matmul(x, self.Wi) + tf.matmul(previous_hidden_state, self.Ui) + self.bi)\n",
        "        # Forget Gate\n",
        "        f = tf.sigmoid(tf.matmul(x, self.Wf) + f.matmul(previous_hidden_state, self.Uf) + self.bf)\n",
        "        # Output Gate\n",
        "        o = tf.sigmoid(tf.matmul(x, self.Wog) + f.matmul(previous_hidden_state, self.Uog) + self.bog)\n",
        "\n",
        "        # New Memory Cell\n",
        "        c_ = tf.nn.tanh(tf.matmul(x, self.Wc) + f.matmul(previous_hidden_state, self.Uc) + self.bc)\n",
        "        # Final Memory cell\n",
        "        c = f * c_prev + i * c_\n",
        "\n",
        "        # Current Hidden state\n",
        "        curr_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "        return tf.stack([cur_hidden_state, c])\n",
        "\n",
        "      return unit\n",
        "\n",
        "  def create_output_unit(self):\n",
        "    self.Wo = tf.identity(self.lstm.Wo)\n",
        "    self.bo = tf.identity(self.lstm.bo)\n",
        "    \n",
        "    def unit(hidden_memory_tuple):\n",
        "      hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "      logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "\n",
        "      return logits\n",
        "\n",
        "    return unit\n",
        "  \n",
        "  def update_output_unit(self):\n",
        "    self.Wo = self.update_rate * self.Wo + (1 - self.update_rate) * tf.identity(self.lstm.Wo)\n",
        "    self.bo = self.update_rate * self.bo + (1 - self.update_rate) * tf.identity(self.lstm.bo)\n",
        "\n",
        "    def unit(hidden_memory_tuple):\n",
        "      hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "      logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "\n",
        "      return logits\n",
        "\n",
        "    return unit\n",
        "  \n",
        "  def update_params(self):\n",
        "    self.g_embeddings = tf.identity(self.lstm.g_embeddings)\n",
        "    self.g_recurrent_unit = self.update_recurrent_unit()\n",
        "    self.g_output_unit = self.update_output_unit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PW7Hyz2J6jro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "e20560e7-37a0-4d46-d84c-29b29d962c86"
      },
      "cell_type": "code",
      "source": [
        "generator = Generator(BATCH_SIZE, SEQ_LENGTH, VOCAB_SIZE, G_EMB_SIZE, \n",
        "                      G_EMB_DIM, G_HIDDEN_DIM, G_START_TOKEN, 0.01, 0.95)\n",
        "\n",
        "rollout = Rollout(generator, 0.8)\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for total_batch in tnrange(1, desc='batch_loop'):\n",
        "  # train generator for one step\n",
        "  for it in range(1):\n",
        "    samples = generator.generate(sess)\n",
        "    rewards = rollout.get_reward(sess, samples, 16, discriminator)\n",
        "    feed = {generator.X_input: samples, generator.rewards: rewards}\n",
        "    _ = sess.run(generator.g_updates, feed_dict=feed)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='1' value='1'></progress>100% 1/1 [02:40&lt;00:00, 160.42s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='16' value='16'></progress>100% 16/16 [02:39&lt;00:00,  9.91s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:10&lt;00:00,  9.60it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 11.30it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 11.38it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00,  9.97it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:10&lt;00:00, 10.87it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 11.35it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 10.03it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00,  9.99it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00,  9.98it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00,  9.97it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 10.02it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 11.44it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 10.03it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 11.28it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 11.43it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='99' value='99'></progress>100% 99/99 [00:09&lt;00:00, 10.02it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tl-aYSCnsBEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "2f822624-37af-4c57-c80f-34e9f6a6f749"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[88 88 88 ... 88 84 97]\n",
            " [51 88 60 ... 98 96 14]\n",
            " [97 33 88 ... 88 14 33]\n",
            " ...\n",
            " [14 60 97 ... 33 88 67]\n",
            " [88 90 83 ... 88 63 63]\n",
            " [97 97 69 ... 94 88 88]]\n",
            "[[0.9799072  0.97886205 0.9790463  ... 0.92636746 0.927194   0.9263591 ]\n",
            " [0.97759557 0.9814943  0.98014665 ... 0.95218873 0.95359933 0.9539719 ]\n",
            " [0.97904336 0.9833619  0.98589325 ... 0.98127097 0.98152435 0.98153496]\n",
            " ...\n",
            " [0.9645319  0.9817629  0.97586524 ... 0.985156   0.98506635 0.9852057 ]\n",
            " [0.9824933  0.97933173 0.9802882  ... 0.99113446 0.9915313  0.9915325 ]\n",
            " [0.9764486  0.97462296 0.976124   ... 0.9808761  0.9803121  0.9798367 ]]\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dfX5w7d_uMBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}