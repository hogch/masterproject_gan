{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqGAN_headlines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogch/masterproject_gan/blob/master/SeqGAN_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sjo87PQzZyc4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Generation using SeqGAN\n",
        "\n",
        "This notebook generates news headlines using the Machine Learning technology GAN (Generative Adversarial Networks)."
      ]
    },
    {
      "metadata": {
        "id": "OhjZVsMcaMqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yo4no6KeYnM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "172272e0-6f3b-4022-a0a0-ef78aac79dd4"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnmI7T2sZAkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e5385df8-6c6a-4fb9-cf85-ff64aa77ed7a"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Masterproject\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Masterproject\n",
            "abcnews-date-text.csv  screenshots\n",
            "Graph\t\t       SeqGAN\n",
            "headlines.csv\t       SeqGAN_headlines_dataloading_experiments.ipynb\n",
            "news-headlines.db      SeqGAN_headlines.ipynb\n",
            "results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vRnegJVw-2YX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install required dependencies manually**"
      ]
    },
    {
      "metadata": {
        "id": "bS58gcTIfiCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "79179b4f-9182-4d63-ed11-22ef61a73a21"
      },
      "cell_type": "code",
      "source": [
        "!pip install tflearn\n",
        "!pip install tqdm\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip\n",
        "!pip install tensorboardcolab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.11.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->tflearn) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     | 481kB 1.3MB/s\n",
            "Building wheels for collected packages: tqdm\n",
            "  Running setup.py bdist_wheel for tqdm ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-u67tyeh3/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n",
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gDvlo_jb_A9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import required modules**"
      ]
    },
    {
      "metadata": {
        "id": "6tvM_ph_8-Xl",
        "colab_type": "code",
        "outputId": "dca4eec4-fa5b-42c7-f89e-9c1d352e749f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import datetime\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tflearn.data_utils import pad_sequences, to_categorical\n",
        "from tensorflow.contrib import slim\n",
        "from tqdm import tqdm, tnrange\n",
        "\n",
        "from tensorboardcolab import *"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4zlW6cONe9YE",
        "colab_type": "code",
        "outputId": "486334f8-97b4-42f2-f888-eb7ed721ac57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JZEIBoWN44Dv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Al3MGHfT46pq",
        "colab_type": "code",
        "outputId": "249b3d3c-455c-4c48-b992-68da9f5af031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "df_real = pd.read_csv('headlines.csv', sep=',', usecols=['text', 'fake'])\n",
        "df_real = df_real.sample(frac=1)\n",
        "df_real = df_real[:50000]\n",
        "print(df_real.shape)\n",
        "df_real.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>64917</th>\n",
              "      <td>qld govt declines powerline green plea</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>880645</th>\n",
              "      <td>us urges china to account for tiananmen crackdown</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17698</th>\n",
              "      <td>tourism delegates to head north west</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598829</th>\n",
              "      <td>tough laws dont deter se dogs</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423240</th>\n",
              "      <td>rudd applauds china economic package</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  fake\n",
              "64917              qld govt declines powerline green plea     0\n",
              "880645  us urges china to account for tiananmen crackdown     0\n",
              "17698                tourism delegates to head north west     0\n",
              "598829                      tough laws dont deter se dogs     0\n",
              "423240               rudd applauds china economic package     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "P1HQ85uyARIJ",
        "colab_type": "code",
        "outputId": "45d63c26-600f-4470-8361-0f2f8a813a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "cell_type": "code",
      "source": [
        "df_fake = pd.DataFrame(columns=['text', 'fake'])\n",
        "print(df_fake.shape)\n",
        "df_fake.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, fake]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "UFCT682iAiwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Test-, Trainingset and Hyper-Parameter"
      ]
    },
    {
      "metadata": {
        "id": "mat90D4fAnPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General Hyper-Parameter\n",
        "BATCH_SIZE = 32\n",
        "VOCAB_SIZE = 2000 # default value (later dynamic assigned with length of word index)\n",
        "SEQ_LENGTH = 8 # average sequence length of the given sentences in the dataset\n",
        "TRAINING_SPLIT = 0.2\n",
        "TOTAL_EPOCH = 200\n",
        "\n",
        "# Discriminator Hyper-Parameter\n",
        "D_PRETRAIN_EPOCHS = 50\n",
        "D_EMB_SIZE = 100\n",
        "D_EMB_DIM = 64\n",
        "D_FILTER_SIZES = [2,3]\n",
        "D_NUM_CLASSES = 2\n",
        "D_NUM_FILTERS = 50\n",
        "\n",
        "# Generator Hyper-Parameter\n",
        "G_PRETRAIN_EPOCHS = 1000\n",
        "G_START_TOKEN = 0\n",
        "G_EMB_SIZE = 100\n",
        "G_EMB_DIM = 32\n",
        "G_HIDDEN_DIM = 32 # hidden state dimension of lstm cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dNP674HF22oU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_datasets(texts, labels, tokenizer=None):\n",
        "  if tokenizer is None:\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  word_index = tokenizer.word_index\n",
        "  text_seq = pad_sequences(sequences, maxlen=SEQ_LENGTH)\n",
        "    \n",
        "  labels = np.asarray(labels)\n",
        "\n",
        "  indices = np.arange(text_seq.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  text_seq = text_seq[indices]\n",
        "  labels = labels[indices]\n",
        "  test_size = int(TRAINING_SPLIT * text_seq.shape[0])\n",
        "  \n",
        "  X_train = text_seq[:test_size]\n",
        "  y_train = to_categorical(labels[:test_size], 2)\n",
        "  X_test = text_seq[test_size:]\n",
        "  y_test = labels[test_size:]\n",
        "  \n",
        "  return tokenizer, word_index, X_train, y_train, X_test, y_test\n",
        "  \n",
        "def load_fake_data():\n",
        "  texts = []\n",
        "  labels = []\n",
        "  \n",
        "  global df_fake\n",
        "  df_fake = df_fake.sample(frac=1)\n",
        "  \n",
        "  for row in zip(df_fake['text'], df_fake['fake']):\n",
        "    texts.append(row[0].strip())\n",
        "    labels.append(row[1])\n",
        "\n",
        "  tokenizer, word_index, X_train, y_train, X_test, y_test = get_datasets(texts, labels)\n",
        "  VOCAB_SIZE = len(word_index) # number of different words in the dataset\n",
        "  \n",
        "  return tokenizer, word_index, X_train, y_train, X_test, y_test\n",
        "\n",
        "def load_real_data():\n",
        "  texts = []\n",
        "  labels = []\n",
        "  \n",
        "  global df_real\n",
        "  df_real = df_real.sample(frac=1)\n",
        "  \n",
        "  for row in zip(df_real['text'], df_real['fake']):\n",
        "    texts.append(row[0].strip())\n",
        "    labels.append(row[1])\n",
        "\n",
        "  tokenizer, word_index, X_train, y_train, X_test, y_test = get_datasets(texts, labels)\n",
        "  VOCAB_SIZE = len(word_index) # number of different words in the dataset\n",
        "  \n",
        "  return tokenizer, word_index, X_train, y_train, X_test, y_test\n",
        "\n",
        "def load_mixed_data():\n",
        "  texts = []\n",
        "  labels = []\n",
        "  \n",
        "  df = pd.concat([df_real, df_fake])\n",
        "  df = df.sample(frac=1)\n",
        "\n",
        "  for row in zip(df['text'], df['fake']):\n",
        "    texts.append(row[0].strip())\n",
        "    labels.append(row[1])\n",
        "\n",
        "  tokenizer, word_index, X_train, y_train, X_test, y_test = get_datasets(texts, labels)\n",
        "  VOCAB_SIZE = len(word_index) # number of different words in the dataset\n",
        "  \n",
        "  return tokenizer, word_index, X_train, y_train, X_test, y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qIo4O_PW1Hfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discriminator\n",
        "model for classifying sequences (here headlines) as real or fake.\n",
        "In this implementation the discriminative model uses following layers: \n",
        "1.   embedding layer\n",
        "2.   convolution layer\n",
        "3.   max-pooling layer\n",
        "4.   softmax layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k3UdGGRc5FfF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator:\n",
        "  def __init__(self, vocab_size, seq_length, emb_size, filter_sizes, num_classes, num_filters, learning_rate):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.seq_length = seq_length\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[None, self.seq_length], name='X_input')\n",
        "    self.y_input = tf.placeholder(tf.int32, shape=[None, self.num_classes], name='y_input')\n",
        "    self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
        "    \n",
        "    # Keeping track of l2 regularization loss (optional)\n",
        "    self.l2_reg_lambda = 0.2\n",
        "    self.l2_loss = tf.constant(0.0)\n",
        "\n",
        "  def build_model(self):\n",
        "    with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):\n",
        "      self.embedding_layer = self.build_embedding_layer()\n",
        "      self.convolution_maxpool_layer = self.build_convolution_maxpool_layer()\n",
        "      self.scores, self.predictions = self.build_softmax_layer()\n",
        "\n",
        "      self.calc_mean_cross_entropy_loss()\n",
        "      self.calc_accuracy_and_cost()\n",
        "\n",
        "      self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
        "      \n",
        "      self.d_summary = tf.summary.merge([\n",
        "          tf.summary.scalar('d_loss', self.loss),\n",
        "          tf.summary.scalar('d_acc', self.accuracy)\n",
        "      ])\n",
        "        \n",
        "  def build_embedding_layer(self):\n",
        "    with tf.device('gpu:0'), tf.name_scope('embedding_layer'):\n",
        "      W_emb = tf.Variable(\n",
        "          initial_value=tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0), \n",
        "          name='W'\n",
        "      )\n",
        "      emb_chars = tf.nn.embedding_lookup(W_emb, self.X_input)\n",
        "      self.emb_chars_expand = tf.expand_dims(emb_chars, -1)\n",
        "    \n",
        "  def build_convolution_maxpool_layer(self):\n",
        "    pooled_outputs = []\n",
        "    for filter_size in self.filter_sizes:\n",
        "      with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
        "        # Convolution Layer\n",
        "        filter_shape = [filter_size, self.emb_size, 1, self.num_filters]\n",
        "        W_filters = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
        "        b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name='b')\n",
        "        conv = tf.nn.conv2d(\n",
        "            input=self.emb_chars_expand,\n",
        "            filter=W_filters,\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='conv'\n",
        "        )\n",
        "        # Apply non-linearity - activation function\n",
        "        activation = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
        "        # Maxpooling over outputs\n",
        "        max_pooling = tf.nn.max_pool(\n",
        "            value=activation,\n",
        "            ksize=[1, self.seq_length-filter_size+1, 1, 1],\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='max_pooling'\n",
        "        )\n",
        "        pooled_outputs.append(max_pooling)\n",
        "              \n",
        "    # combine all the pooled features\n",
        "    self.num_filter_total = self.num_filters * len(self.filter_sizes)\n",
        "    h_pool = tf.concat(pooled_outputs, axis=3)\n",
        "    return tf.reshape(h_pool, [-1, self.num_filter_total])\n",
        "        \n",
        "  def build_softmax_layer(self): \n",
        "    with tf.name_scope('highway'):\n",
        "      self.h_highway = self.highway(\n",
        "          self.convolution_maxpool_layer, self.convolution_maxpool_layer.get_shape()[1], 1, 0\n",
        "      )\n",
        "\n",
        "    with tf.name_scope('dropout'):\n",
        "      self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
        "      \n",
        "    with tf.name_scope('softmax_output'):\n",
        "      W_softmax = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              [self.num_filter_total, self.num_classes], \n",
        "              stddev=0.1\n",
        "          ), name='W_softmax'\n",
        "      )\n",
        "      b_softmax = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name='b_softmax')\n",
        "            \n",
        "      self.l2_loss += tf.nn.l2_loss(W_softmax)\n",
        "      self.l2_loss += tf.nn.l2_loss(b_softmax)\n",
        "      \n",
        "      self.scores = tf.nn.xw_plus_b(self.h_drop, W_softmax, b_softmax, name='scores')\n",
        "      #self.scores = tf.matmul(self.convolution_maxpool_layer, W_softmax) + b_softmax\n",
        "      self.truth_prob = tf.nn.softmax(self.scores)\n",
        "      predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "      \n",
        "    return self.scores, predictions\n",
        "  \n",
        "  def calc_mean_cross_entropy_loss(self):\n",
        "    with tf.name_scope('mse_loss'):\n",
        "      losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=self.y_input)\n",
        "      self.loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
        "      \n",
        "  def calc_accuracy_and_cost(self):\n",
        "    with tf.name_scope('accuracy'):\n",
        "      correct_predictions = tf.equal(self.predictions, tf.argmax(self.y_input, 1))\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
        "      self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=self.y_input))\n",
        "      \n",
        "  def highway(self, input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n",
        "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
        "    t = sigmoid(Wy + b)\n",
        "    z = t * g(Wy + b) + (1 - t) * y\n",
        "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
        "    \"\"\"\n",
        "    with tf.variable_scope('highway'):\n",
        "      size = int(size)\n",
        "      for idx in range(num_layers):\n",
        "        g = f(slim.fully_connected(input_, size, scope='highway_lin_%d' % idx, activation_fn=None))\n",
        "        t = tf.sigmoid(slim.fully_connected(input_, size, scope='highway_gate_%d' % idx, activation_fn=None) + bias)\n",
        "\n",
        "        output = t * g + (1. - t) * input_\n",
        "        input_ = output\n",
        "        \n",
        "    return output\n",
        "  \n",
        "  def get_truth_prob(self, sess, X):\n",
        "    feed_dict = { self.X_input: X, self.dropout_keep_prob: 1.0 }\n",
        "        \n",
        "    return sess.run(self.truth_prob, feed_dict=feed_dict)\n",
        "        \n",
        "  def train(self, sess, X, y, dropout):\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    feed_dict = {\n",
        "        self.X_input: X,\n",
        "        self.y_input: y,\n",
        "        self.dropout_keep_prob: dropout\n",
        "    }\n",
        "    _, summary = sess.run([self.optimizer, self.d_summary], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B86yDcx3TC4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Discriminator model and train model"
      ]
    },
    {
      "metadata": {
        "id": "UnVPueM8TXwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "discriminator = Discriminator(VOCAB_SIZE, SEQ_LENGTH, D_EMB_SIZE, D_FILTER_SIZES, \n",
        "                              D_NUM_CLASSES, D_NUM_FILTERS, 0.001)\n",
        "discriminator.build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmjkZBeH557j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "\n",
        "LSTM with Reinforcement Learning for sequence generation."
      ]
    },
    {
      "metadata": {
        "id": "lcALithWy39s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib import rnn, seq2seq, slim\n",
        "from __future__ import print_function\n",
        "\n",
        "class Generator:\n",
        "  def __init__(self, batch_size, seq_length, vocab_size, emb_size, emb_dim, \n",
        "               hidden_dim, start_token, learning_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_length = seq_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.start_token = start_token\n",
        "    self.learning_rate = learning_rate\n",
        "    self.grad_clip = 5.0\n",
        "    \n",
        "    self.given_tokens = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length], name='given_tokens')\n",
        "    self.start_tokens = tf.Variable(tf.tile([self.start_token], [self.batch_size]), name='start_tokens')\n",
        "    self.decoder_lengths = tf.placeholder(tf.int32, shape=[self.batch_size], name='decoder_lengths')\n",
        "    \n",
        "    with tf.variable_scope('generator', reuse=tf.AUTO_REUSE):\n",
        "      self.g_rnn = rnn.LSTMCell(hidden_dim)\n",
        "      self.g_embeddings = tf.Variable(tf.random_normal([self.vocab_size, self.emb_dim], stddev=0.1)) # init embeddings matrix\n",
        "      self.g_embedding_fn = tf.nn.embedding_lookup(self.g_embeddings, self.given_tokens)\n",
        "      \n",
        "      self.decision_W = tf.Variable(tf.random_normal([self.hidden_dim, self.vocab_size]), name='decision_W')\n",
        "      self.decision_b = tf.Variable(tf.zeros([self.vocab_size]), name='decision_b')\n",
        "      \n",
        "      self.output_ids = []\n",
        "      self.output_probs = []\n",
        "            \n",
        "      for i in range(self.seq_length+1):\n",
        "        self.decoder_lengths = np.int32(np.ones((self.batch_size), dtype=int) * self.seq_length)\n",
        "        helper = seq2seq.TrainingHelper(self.g_embedding_fn, self.decoder_lengths, time_major=False)\n",
        "        decoder = seq2seq.BasicDecoder(\n",
        "            cell=self.g_rnn, helper=helper, \n",
        "            initial_state=self.g_rnn.zero_state(self.batch_size, tf.float32)\n",
        "        )\n",
        "        # https://gist.github.com/higepon/eb81ba0f6663a57ff1908442ce753084\n",
        "        # Dynamic decoding\n",
        "        # final_outputs: rnn_output=list of RNN state, sample_id=list of argmax of rnn_output\n",
        "        # final_state: list of final state of RNN on decode process\n",
        "        # final_seq_lengths: list of each decoded sequence\n",
        "        final_outputs, final_state, final_seq_lengths = \\\n",
        "            seq2seq.dynamic_decode(decoder)\n",
        "\n",
        "        self.output_ids.append(final_outputs.sample_id)\n",
        "        self.output_probs.append(tf.nn.softmax(\n",
        "            tf.tensordot(final_outputs.rnn_output, self.decision_W, axes=[[2],[0]])\n",
        "            + self.decision_b[None,None,:])\n",
        "        )\n",
        "    \n",
        "      # PRETRAIN\n",
        "      # all tokens from given sequence\n",
        "      logit = final_outputs.rnn_output\n",
        "      self.pretrain_loss = tf.reduce_mean(\n",
        "          tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.given_tokens, logits=logit)\n",
        "      )\n",
        "      self.pretrain_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "      self.pretrain_op = slim.learning.create_train_op(\n",
        "          self.pretrain_loss, self.pretrain_optimizer, clip_gradient_norm=5.0\n",
        "      )\n",
        "      self.pretrain_summary = tf.summary.scalar('g_pretrain_loss', self.pretrain_loss)\n",
        "\n",
        "      # reinforcement learning\n",
        "      self.rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.seq_length], name='rewards')\n",
        "      g_seq = self.output_ids[self.seq_length] # follow the generated one\n",
        "      g_prob = self.output_probs[self.seq_length]\n",
        "      g_loss = -tf.reduce_mean(\n",
        "          tf.reduce_sum(\n",
        "              tf.one_hot(g_seq, self.vocab_size) * \n",
        "              tf.log(tf.clip_by_value(g_prob, 1e-8, 1-1e-8)), -1\n",
        "          ) * self.rewards\n",
        "      )\n",
        "      g_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "      self.g_op = slim.learning.create_train_op(g_loss, g_optimizer, clip_gradient_norm=5.0)\n",
        "      self.g_summary = tf.summary.merge([\n",
        "          tf.summary.scalar('g_loss', g_loss),\n",
        "          tf.summary.scalar('g_reward', tf.reduce_mean(self.rewards))\n",
        "      ])\n",
        "      self.image_summary = tf.summary.merge([\n",
        "          tf.summary.image('real_samples', tf.expand_dims(\n",
        "              tf.one_hot(self.given_tokens, self.vocab_size), -1)\n",
        "          ),\n",
        "          tf.summary.image('fake_samples', tf.expand_dims(\n",
        "              tf.one_hot(self.output_ids[0], self.vocab_size), -1)\n",
        "          )\n",
        "      ])\n",
        "  \n",
        "  def generate(self, sess, given_tokens):\n",
        "    feed_dict = { self.given_tokens: given_tokens }\n",
        "    \n",
        "    return sess.run(self.output_ids[0], feed_dict=feed_dict)\n",
        "      \n",
        "  def rollout(self, sess, given_tokens, keep_steps=0, with_probs=False):\n",
        "    feed_dict = { self.given_tokens: given_tokens }\n",
        "    if with_probs:\n",
        "      output_tensors = [self.output_ids[keep_steps], self.output_probs[keep_steps]]\n",
        "    else:\n",
        "      output_tensors = self.output_ids[keep_steps]\n",
        "      \n",
        "    return sess.run(output_tensors, feed_dict=feed_dict)\n",
        "  \n",
        "  def get_reward(self, sess, given_tokens, rollout_num, dis):\n",
        "    rewards = []\n",
        "    for i in range(rollout_num):\n",
        "      for keep_num in range(1, self.seq_length+1):    \n",
        "        # Markov Chain Sample / Monte Carlo Sample??\n",
        "        mc_sample = self.rollout(sess, given_tokens, keep_steps=keep_num)\n",
        "        truth_prob = dis.get_truth_prob(sess, mc_sample)\n",
        "        ypred = np.array([item[1] for item in truth_prob])\n",
        "        if i == 0:\n",
        "          rewards.append(ypred)\n",
        "        else:\n",
        "          rewards[keep_num-1] += ypred\n",
        "        \n",
        "    rewards = np.transpose(np.array(rewards)) / (1.0 * rollout_num)\n",
        "    \n",
        "    return rewards\n",
        "  \n",
        "  def pretrain(self, sess, given_tokens):\n",
        "    feed_dict = { self.given_tokens: given_tokens }\n",
        "    sess.run(self.pretrain_op, feed_dict=feed_dict)\n",
        "    _, summary = sess.run([self.pretrain_op, self.pretrain_summary], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary\n",
        "  \n",
        "  def train(self, sess, given_tokens, rewards):\n",
        "    feed_dict = { self.given_tokens: given_tokens, self.rewards: rewards }\n",
        "    _, summary = sess.run([self.g_op, self.g_summary], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V03XEe_uVdwq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Generator model"
      ]
    },
    {
      "metadata": {
        "id": "PW7Hyz2J6jro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = Generator(BATCH_SIZE, SEQ_LENGTH, VOCAB_SIZE, G_EMB_SIZE, \n",
        "                      G_EMB_DIM, G_HIDDEN_DIM, G_START_TOKEN, 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uccb8c0sViK5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Start Adversarial Training"
      ]
    },
    {
      "metadata": {
        "id": "hN-drlXj1Kyj",
        "colab_type": "code",
        "outputId": "0acf9bd8-c301-4127-aa26-1f477a15db16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "tbc=TensorBoardColab()\n",
        "\n",
        "LOG_DIR = '/tmp/log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://a003a35f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ymHD3nwRtS7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_text(tokenizer, samples):\n",
        "  reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "  \n",
        "  # Function takes a tokenized sentence and returns the words\n",
        "  def sequence_to_text(samples):\n",
        "      # Looking up words in dictionary\n",
        "      words = [reverse_word_map.get(letter) for letter in samples]\n",
        "      return words\n",
        "\n",
        "  # Creating texts \n",
        "  samples_text = list(map(sequence_to_text, samples))\n",
        "  return samples_text\n",
        "\n",
        "def add_fake_samples(tokenizer, samples):\n",
        "  samples_text = get_text(tokenizer, samples)\n",
        "  \n",
        "  headlines = []\n",
        "  for item in samples_text:\n",
        "    headline = ''\n",
        "    for word in item:\n",
        "      headline += '%s ' % word\n",
        "\n",
        "    global df_fake\n",
        "    df_fake = df_fake.append({'text' : headline , 'fake' : 1} , ignore_index=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GbdSazLhVllp",
        "colab_type": "code",
        "outputId": "aa99d7af-6099-4b16-fa64-ac87fc432338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  print('start GAN training at', datetime.datetime.now())\n",
        "  writer = tbc.get_writer()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  \"\"\"# PRETRAINING GENERATOR\n",
        "  for epoch in range(G_PRETRAIN_EPOCHS):\n",
        "    # seq_length -1 und von 1 anfangen\n",
        "    tokenizer, word_index, X_train, y_train, X_test, y_test = load_mixed_data()\n",
        "    summary = generator.pretrain(sess, X_train[:BATCH_SIZE])\n",
        "    writer.add_summary(summary, epoch)\n",
        "    \n",
        "  # PRETRAIN DISCRIMINATOR\n",
        "  for eoch in range(D_PRETRAIN_EPOCHS):\n",
        "    fake_samples = generator.generator(sess)\n",
        "    translate_samples(tokenizer, fake_samples, df_fake)\n",
        "    tokenizer, word_index, X_train, y_train, X_test, y_test = load_data()\n",
        "    \n",
        "    for _ in range(3):\n",
        "      discriminator.train(sess, X_train, y_train, 5, BATCH_SIZE, .001)\"\"\"\n",
        "\n",
        "  # ADVERSARIAL TRAINING\n",
        "  for epoch in tnrange(TOTAL_EPOCH, desc='gan_epoch_loop'):\n",
        "    tokenizer, word_index, X_train, y_train, X_test, y_test = load_real_data()\n",
        "    # train generator for one step\n",
        "    for it in range(1):\n",
        "      fake_samples = generator.generate(sess, X_train[:BATCH_SIZE])\n",
        "      rewards = generator.get_reward(sess, fake_samples, 16, discriminator)\n",
        "      summary = generator.train(sess, fake_samples, rewards)\n",
        "    \n",
        "    writer.add_summary(summary, epoch)\n",
        "    \n",
        "    for _ in tnrange(5, desc='gen_train_loop'):\n",
        "      tokenizer, word_index, X_train, y_train, X_test, y_test = load_real_data()\n",
        "      fake_samples = generator.generate(sess, X_train[:BATCH_SIZE])\n",
        "      add_fake_samples(tokenizer, fake_samples)\n",
        "      \n",
        "      for _ in tnrange(3, desc='dis_train_loop'):\n",
        "        tokenizer, word_index, X_train, y_train, X_test, y_test = load_mixed_data()\n",
        "        summary = discriminator.train(sess, X_train[:BATCH_SIZE], y_train[:BATCH_SIZE], 0.4)\n",
        "        \n",
        "    print(df_fake.tail(2))\n",
        "    writer.add_summary(summary, epoch)\n",
        "    \n",
        "    \"\"\"tokenizer, word_index, X_real_train, y_real_train, X_real_test, y_real_test = load_real_data()\n",
        "    summary = sess.run(generator.image_summary, feed_dict={ generator.given_tokens: X_real_train[:BATCH_SIZE] })\n",
        "    writer.add_summary(summary, epoch)\"\"\"\n",
        "    \n",
        "  print('finish GAN training at', datetime.datetime.now())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start GAN training at 2018-12-20 11:32:46.534621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gan_epoch_loop</span><progress style='margin:2px 4px;description_width:initial;' max='200' value='1'></progress>  0% 1/200 [00:49&lt;2:45:46, 49.98s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gen_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='5' value='5'></progress>100% 5/5 [00:43&lt;00:00,  8.70s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:06&lt;00:00,  2.36s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:06&lt;00:00,  2.17s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:06&lt;00:00,  2.25s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:06&lt;00:00,  2.14s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>dis_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='3' value='3'></progress>100% 3/3 [00:06&lt;00:00,  2.20s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                          text fake\n",
            "158  None None with on on not council council     1\n",
            "159  None man us over man None govt interview     1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>gen_train_loop</span><progress style='margin:2px 4px;description_width:initial;' max='5' value='0'></progress>  0% 0/5 [00:00&lt;?, ?it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5b941d7568bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtnrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gen_train_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_real_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m       \u001b[0mfake_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0madd_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-53e60c631482>\u001b[0m in \u001b[0;36mload_real_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m   \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m   \u001b[0mVOCAB_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# number of different words in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-53e60c631482>\u001b[0m in \u001b[0;36mget_datasets\u001b[0;34m(texts, labels, tokenizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    220\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtranslate_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2TgllHrMIdaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "print(df_fake.tail())\n",
        "\n",
        "count = 1\n",
        "saved = False\n",
        "while saved != True:\n",
        "  filename = 'generated_headlines_%d.csv' % count\n",
        "  filepath = Path('./results/%s' % filename)\n",
        "  \n",
        "  if filepath.exists():\n",
        "    count = count+1\n",
        "  else:\n",
        "    df_fake.to_csv(filepath, sep='\\t', encoding='utf-8')\n",
        "    saved = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nNXWnagxP-8Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}