{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqGAN_headlines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogch/masterproject_gan/blob/train-discriminator/SeqGAN_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sjo87PQzZyc4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Generation using GAN\n",
        "\n",
        "This notebook generates news headlines using the Machine Learning technology GAN (Generative Adversarial Networks)."
      ]
    },
    {
      "metadata": {
        "id": "OhjZVsMcaMqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yo4no6KeYnM3",
        "colab_type": "code",
        "outputId": "01e54494-39bc-4497-8f83-b1a750effeab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnmI7T2sZAkv",
        "colab_type": "code",
        "outputId": "1dd4095a-70fc-41a9-b338-341da742942e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Masterproject\n",
        "!ls"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Masterproject\n",
            "abcnews-date-text.csv\t news-headlines-short.db-journal\n",
            "fake_data.txt\t\t positive_file.txt\n",
            "headlines.csv\t\t real_data.txt\n",
            "headlines-short.csv\t SeqGAN\n",
            "imdb.pkl\t\t SeqGAN_headlines_dataloading_experiments.ipynb\n",
            "news-headlines.db\t SeqGAN_headlines.ipynb\n",
            "news-headlines-short.db\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gDvlo_jb_A9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import required modules**"
      ]
    },
    {
      "metadata": {
        "id": "6tvM_ph_8-Xl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vRnegJVw-2YX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install required dependencies manually**"
      ]
    },
    {
      "metadata": {
        "id": "JZEIBoWN44Dv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Al3MGHfT46pq",
        "colab_type": "code",
        "outputId": "042b7d39-6592-4b8a-9673-19b453438ce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('headlines-short.csv', sep=',', index_col='id')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20030219</td>\n",
              "      <td>aba decides against community broadcasting lic...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20030219</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20030219</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    publish_date                                               text  fake\n",
              "id                                                                       \n",
              "1       20030219  aba decides against community broadcasting lic...     0\n",
              "2       20030219     act fire witnesses must be aware of defamation     0\n",
              "3       20030219     a g calls for infrastructure protection summit     0\n",
              "4       20030219           air nz staff in aust strike for pay rise     0\n",
              "5       20030219      air nz strike to affect australian travellers     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "UFCT682iAiwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Test-, Trainingset and Hyper-Parameter for the Discriminator"
      ]
    },
    {
      "metadata": {
        "id": "mat90D4fAnPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Discriminator Hyper-Parameter\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = 16000 # 20\n",
        "SEQ_LENGTH = 100\n",
        "EMB_SIZE = 100\n",
        "EMB_DIM = 64\n",
        "FILTER_SIZES = [2,3]\n",
        "NUM_CLASSES = 2\n",
        "NUM_FILTERS = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2mWyXypVQMra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "8cf38e18-311e-491e-a047-c3829911a96c"
      },
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for row in zip(df['text'], df['fake']):\n",
        "  texts.append(row[0])\n",
        "  labels.append(row[1])\n",
        "  \n",
        "dis_dataloader.load_train_data(texts, labels)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-371a9a9141d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdis_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-70-3dceeaebaa45>\u001b[0m in \u001b[0;36mload_train_data\u001b[0;34m(self, texts, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           \u001b[0mparse_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m           \u001b[0mpositive_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \"\"\"with open(negative_file)as fin:\n",
            "\u001b[0;32m<ipython-input-70-3dceeaebaa45>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           \u001b[0mparse_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m           \u001b[0mpositive_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \"\"\"with open(negative_file)as fin:\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'aba'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "vEBAdcJXRZqN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1462
        },
        "outputId": "ee652176-fd1f-4a63-f1e8-93e1496f4f43"
      },
      "cell_type": "code",
      "source": [
        "from tflearn.datasets import imdb\n",
        "from tflearn.data_utils import pad_sequences, to_categorical    \n",
        "  \n",
        "(X_train, y_train), (X_test, y_test), _ = imdb.load_data()\n",
        "X_train = np.array(pad_sequences(X_train, maxlen=100))\n",
        "\n",
        "X_test = np.array(pad_sequences(X_test, maxlen=100))\n",
        "\n",
        "vocab_size = X_train.max() + 1\n",
        "print('vocab size:', format(vocab_size))\n",
        "y_train = to_categorical(np.array(y_train), 2)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "discriminator = Discriminator(vocab_size, SEQ_LENGTH, EMB_SIZE,\n",
        "                              FILTER_SIZES, NUM_CLASSES, NUM_FILTERS)\n",
        "discriminator.build_model()\n",
        "discriminator.train(X_train, y_train, 5, BATCH_SIZE, .001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size: 99999\n",
            "(64, 2)\n",
            "(128, 2)\n",
            "(192, 2)\n",
            "(256, 2)\n",
            "(320, 2)\n",
            "(384, 2)\n",
            "(448, 2)\n",
            "(512, 2)\n",
            "(576, 2)\n",
            "(640, 2)\n",
            "(704, 2)\n",
            "(768, 2)\n",
            "(832, 2)\n",
            "(896, 2)\n",
            "(960, 2)\n",
            "(1024, 2)\n",
            "(1088, 2)\n",
            "(1152, 2)\n",
            "(1216, 2)\n",
            "(1280, 2)\n",
            "(1344, 2)\n",
            "(1408, 2)\n",
            "(1472, 2)\n",
            "(1536, 2)\n",
            "(1600, 2)\n",
            "(1664, 2)\n",
            "(1728, 2)\n",
            "(1792, 2)\n",
            "(1856, 2)\n",
            "(1920, 2)\n",
            "(1984, 2)\n",
            "(2048, 2)\n",
            "(2112, 2)\n",
            "(2176, 2)\n",
            "(2240, 2)\n",
            "(2304, 2)\n",
            "(2368, 2)\n",
            "(2432, 2)\n",
            "(2496, 2)\n",
            "(2560, 2)\n",
            "(2624, 2)\n",
            "(2688, 2)\n",
            "(2752, 2)\n",
            "(2816, 2)\n",
            "(2880, 2)\n",
            "(2944, 2)\n",
            "(3008, 2)\n",
            "(3072, 2)\n",
            "(3136, 2)\n",
            "(3200, 2)\n",
            "(3264, 2)\n",
            "(3328, 2)\n",
            "(3392, 2)\n",
            "(3456, 2)\n",
            "(3520, 2)\n",
            "(3584, 2)\n",
            "(3648, 2)\n",
            "(3712, 2)\n",
            "(3776, 2)\n",
            "(3840, 2)\n",
            "(3904, 2)\n",
            "(3968, 2)\n",
            "(4032, 2)\n",
            "(4096, 2)\n",
            "(4160, 2)\n",
            "(4224, 2)\n",
            "(4288, 2)\n",
            "(4352, 2)\n",
            "(4416, 2)\n",
            "(4480, 2)\n",
            "(4544, 2)\n",
            "(4608, 2)\n",
            "(4672, 2)\n",
            "(4736, 2)\n",
            "(4800, 2)\n",
            "(4864, 2)\n",
            "(4928, 2)\n",
            "(4992, 2)\n",
            "(5056, 2)\n",
            "(5120, 2)\n",
            "(5184, 2)\n",
            "(5248, 2)\n",
            "(5312, 2)\n",
            "(5376, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8UtHsUHe_cX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Dis_dataloader():\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.sentences = np.array([])\n",
        "        self.labels = np.array([])\n",
        "\n",
        "    def load_train_data(self, positive_file, negative_file):\n",
        "        # Load data\n",
        "        positive_examples = []\n",
        "        negative_examples = []\n",
        "        with open(positive_file)as fin:\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                line = line.split()\n",
        "                parse_line = [int(x) for x in line]\n",
        "                positive_examples.append(parse_line)\n",
        "        with open(negative_file)as fin:\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                line = line.split()\n",
        "                parse_line = [int(x) for x in line]\n",
        "                if len(parse_line) == 20:\n",
        "                    negative_examples.append(parse_line)\n",
        "        self.sentences = np.array(positive_examples + negative_examples)\n",
        "\n",
        "        # Generate labels\n",
        "        positive_labels = [[0, 1] for _ in positive_examples]\n",
        "        negative_labels = [[1, 0] for _ in negative_examples]\n",
        "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
        "\n",
        "        # Shuffle the data\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(self.labels)))\n",
        "        self.sentences = self.sentences[shuffle_indices]\n",
        "        self.labels = self.labels[shuffle_indices]\n",
        "\n",
        "        # Split batches\n",
        "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
        "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
        "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
        "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
        "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
        "\n",
        "        self.pointer = 0\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
        "        self.pointer = (self.pointer + 1) % self.num_batch\n",
        "        return ret\n",
        "\n",
        "    def reset_pointer(self):\n",
        "        self.pointer = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qIo4O_PW1Hfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discriminator\n",
        "model for classifying sequences (here headlines) as real or fake.\n",
        "In this implementation the discriminative model uses following layers: \n",
        "1.   embedding layer\n",
        "2.   convolution layer\n",
        "3.   max-pooling layer\n",
        "4.   softmax layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k3UdGGRc5FfF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator:\n",
        "  def __init__(self, vocab_size, seq_length, emb_size, filter_sizes, num_classes, num_filters):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.seq_length = seq_length\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "\n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[None, self.seq_length], name='X_input')\n",
        "    self.y_input = tf.placeholder(tf.float32, shape=[None, self.num_classes], name='y_input')\n",
        "    #self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
        "    \n",
        "    # Keeping track of l2 regularization loss (optional)\n",
        "    self.l2_reg_lambda = 0.0\n",
        "    self.l2_loss = tf.constant(0.0)\n",
        "\n",
        "  def build_model(self):\n",
        "    self.embedding_layer = self.build_embedding_layer()\n",
        "    self.convolution_maxpool_layer = self.build_convolution_maxpool_layer()\n",
        "    self.scores, self.predictions = self.build_softmax_layer()\n",
        "    \n",
        "    self.calc_mean_cross_entropy_loss()\n",
        "    self.calc_accuracy_and_cost()\n",
        "        \n",
        "  def build_embedding_layer(self):\n",
        "    with tf.device('cpu:0'), tf.name_scope('embedding_layer'):\n",
        "      W_emb = tf.Variable(\n",
        "          initial_value=tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0), \n",
        "          name='W'\n",
        "      )\n",
        "      emb_chars = tf.nn.embedding_lookup(W_emb, self.X_input)\n",
        "      self.emb_chars_expand = tf.expand_dims(emb_chars, -1)\n",
        "    \n",
        "  def build_convolution_maxpool_layer(self):\n",
        "    pooled_outputs = []\n",
        "    for filter_size in self.filter_sizes:\n",
        "      with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
        "        # Convolution Layer\n",
        "        filter_shape = [filter_size, self.emb_size, 1, self.num_filters]\n",
        "        W_filters = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
        "        b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name='b')\n",
        "        conv = tf.nn.conv2d(\n",
        "            input=self.emb_chars_expand,\n",
        "            filter=W_filters,\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='conv'\n",
        "        )\n",
        "        # Apply non-linearity - activation function\n",
        "        activation = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
        "        # Maxpooling over outputs\n",
        "        max_pooling = tf.nn.max_pool(\n",
        "            value=activation,\n",
        "            ksize=[1, self.seq_length-filter_size+1, 1, 1],\n",
        "            strides=[1,1,1,1],\n",
        "            padding='VALID',\n",
        "            name='max_pooling'\n",
        "        )\n",
        "        pooled_outputs.append(max_pooling)\n",
        "              \n",
        "    self.num_filter_total = self.num_filters * len(self.filter_sizes)\n",
        "    h_pool = tf.concat(pooled_outputs, axis=3)\n",
        "    return tf.reshape(h_pool, [-1, self.num_filter_total])\n",
        "        \n",
        "  def build_softmax_layer(self):\n",
        "    \"\"\"with tf.name_scope('dropout'): # without highway function\n",
        "      self.h_drop = tf.nn.dropout(\n",
        "          self.convolution_maxpool_layer, \n",
        "          self.dropout_keep_prob\n",
        "      )\"\"\"\n",
        "    with tf.name_scope('softmax_output'):\n",
        "      W_softmax = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              [self.num_filter_total, self.num_classes], \n",
        "              stddev=0.1\n",
        "          ), name='W'\n",
        "      )\n",
        "      b_softmax = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name='b')\n",
        "            \n",
        "      self.l2_loss += tf.nn.l2_loss(W_softmax)\n",
        "      self.l2_loss += tf.nn.l2_loss(b_softmax)\n",
        "      \n",
        "      #self.scores = tf.nn.xw_plus_b(self.h_drop, W_softmax, b_softmax, name='scores')\n",
        "      self.scores = tf.matmul(self.convolution_maxpool_layer, W_softmax) + b_softmax\n",
        "      predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "      \n",
        "    return self.scores, predictions\n",
        "  \n",
        "  def calc_mean_cross_entropy_loss(self):\n",
        "    with tf.name_scope('cross_entropy_loss'):\n",
        "      losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.y_input)\n",
        "      self.loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
        "      \n",
        "  def calc_accuracy_and_cost(self):\n",
        "    with tf.name_scope('accuracy'):\n",
        "      correct_predictions = tf.equal(self.predictions, tf.argmax(self.y_input, 1))\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
        "      self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.y_input))\n",
        "  \n",
        "  def train(self, X, y, num_epochs, batch_size, learning_rate):\n",
        "    with tf.name_scope('loss'):\n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.initialize_all_variables())\n",
        "      epoch_i = 0\n",
        "      \n",
        "      while epoch_i < num_epochs:\n",
        "        batch_i = 0\n",
        "        batch_losses = []\n",
        "        batch_accs = []\n",
        "            \n",
        "        for i in range(batch_size, X.shape[0], batch_size):\n",
        "          X_batch, y_batch = X[batch_i:i], y[batch_i:i]\n",
        "          print(y_batch.shape)\n",
        "          feed_dict = {\n",
        "              self.X_input: X_batch,\n",
        "              self.y_input: y_batch\n",
        "          }\n",
        "          sess.run(optimizer, feed_dict)\n",
        "          loss, accuracy = sess.run([self.cost, self.accuracy], feed_dict)\n",
        "    \n",
        "        time_str = datetime.datetime.now().isoformat()\n",
        "        print(\"{}: step {}, loss {}, acc {}\".format(time_str, i, str(self.loss), str(self.accuracy)))\n",
        "        epoch_i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B86yDcx3TC4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Discriminator model and Dataloader"
      ]
    },
    {
      "metadata": {
        "id": "UnVPueM8TXwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator(VOCAB_SIZE, SEQ_LENGTH, EMB_SIZE,\n",
        "                              FILTER_SIZES, NUM_CLASSES, NUM_FILTERS)\n",
        "\n",
        "discriminator.build_model()\n",
        "\n",
        "dis_dataloader = Dis_dataloader(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXL1CrhMk3n0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-train and Train Discriminator model"
      ]
    },
    {
      "metadata": {
        "id": "c9JSsO4lk7tP",
        "colab_type": "code",
        "outputId": "151eca7b-8ba5-4541-9ef1-2c2d6c7a2f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "cell_type": "code",
      "source": [
        "discriminator.train(dis_dataloader, 5, BATCH_SIZE, .001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  99999\n",
            "(0, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-24d62802a5a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-302300bb44e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    119\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m           }\n\u001b[0;32m--> 121\u001b[0;31m           \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                             \u001b[0;34m'For reference, the tensor object was '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' which was passed to the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                             'feed with key ' + str(feed) + '.')\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m           \u001b[0msubfeed_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.For reference, the tensor object was Tensor(\"dropout_keep_prob_2:0\", dtype=float32) which was passed to the feed with key Tensor(\"dropout_keep_prob_2:0\", dtype=float32)."
          ]
        }
      ]
    }
  ]
}