{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqGAN_headlines_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogch/masterproject_gan/blob/master/SeqGAN_headlines_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sjo87PQzZyc4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Generation using SeqGAN\n",
        "\n",
        "This notebook generates news headlines using the Machine Learning technology GAN (Generative Adversarial Networks)."
      ]
    },
    {
      "metadata": {
        "id": "OhjZVsMcaMqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yo4no6KeYnM3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OnmI7T2sZAkv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Masterproject\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vRnegJVw-2YX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install required dependencies manually**"
      ]
    },
    {
      "metadata": {
        "id": "bS58gcTIfiCx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tflearn\n",
        "!pip install tqdm\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip\n",
        "!pip install tensorboardcolab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gDvlo_jb_A9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import required modules**"
      ]
    },
    {
      "metadata": {
        "id": "6tvM_ph_8-Xl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import datetime\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tflearn.data_utils import pad_sequences, to_categorical\n",
        "from tensorflow.contrib import slim\n",
        "from tqdm import tqdm, tnrange\n",
        "\n",
        "from tensorboardcolab import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zlW6cONe9YE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JZEIBoWN44Dv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Al3MGHfT46pq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"df_real = pd.read_csv('headlines.csv', sep=',', usecols=['text', 'fake'])\n",
        "df_real = df_real.sample(frac=1)\n",
        "df_real = df_real[:500000]\n",
        "print(df_real.shape)\n",
        "df_real.head()\"\"\"\n",
        "\n",
        "df_real = pd.read_csv('headlines_short.csv', sep=',', usecols=['text', 'fake'])\n",
        "df_real = df_real.sample(frac=1)\n",
        "print(df_real.shape)\n",
        "df_real.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P1HQ85uyARIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_fake = pd.DataFrame(columns=['text', 'fake'])\n",
        "df_evaluation = pd.DataFrame(columns=['text', 'fake'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UFCT682iAiwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Test-, Trainingset and Hyper-Parameter"
      ]
    },
    {
      "metadata": {
        "id": "mat90D4fAnPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General Hyper-Parameter\n",
        "BATCH_SIZE = 64\n",
        "SEQ_LENGTH = 5 # average sequence length of the given sentences in the dataset\n",
        "TRAINING_SPLIT = 1\n",
        "TOTAL_EPOCHS = 200\n",
        "\n",
        "# Discriminator Hyper-Parameter\n",
        "D_PRETRAIN_EPOCHS = 50\n",
        "D_EPOCHS = 1#3\n",
        "D_EMB_SIZE = 300\n",
        "D_NUM_CLASSES = 2\n",
        "D_FILTER_SIZES = [1,2,3,4,5]\n",
        "D_NUM_FILTERS = 128\n",
        "D_DROPOUT = 0.5\n",
        "D_LEARNING_RATE = 0.001\n",
        "\n",
        "# Generator Hyper-Parameter\n",
        "G_PRETRAIN_EPOCHS = 100\n",
        "G_EPOCHS = 10#5\n",
        "G_EMB_SIZE = 300\n",
        "G_HIDDEN_LAYER_SIZES = [64] # hidden state dimension of lstm cell\n",
        "G_TEMPERATURE = 0.5\n",
        "G_DROPOUT = 0.5\n",
        "G_LEARNING_RATE = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xzlveGkqehSq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(df):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  \n",
        "  for row in zip(df['text'], df['fake']):\n",
        "    texts.append(row[0].strip())\n",
        "    labels.append(row[1])\n",
        "  \n",
        "  return texts, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMsZPbs5VFu2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(lower=False)\n",
        "texts, labels = load_data(df_real)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "#sorted_word_count = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "#WORD_INDEX_BIG = [word for idx, word in enumerate(tokenizer.word_index)]\n",
        "WORD_INDEX = [w for w, c in tokenizer.word_counts.items() if c > 5]\n",
        "\n",
        "VOCAB_SIZE = len(WORD_INDEX)\n",
        "\n",
        "print(VOCAB_SIZE)\n",
        "print(WORD_INDEX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dNP674HF22oU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_datasets(texts, labels):\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  text_seq = pad_sequences(sequences, maxlen=SEQ_LENGTH)\n",
        "    \n",
        "  labels = np.asarray(labels)\n",
        "  indices = np.arange(text_seq.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  text_seq = text_seq[indices]\n",
        "  labels = labels[indices]\n",
        "  \n",
        "  test_size = int(TRAINING_SPLIT * text_seq.shape[0])\n",
        "  \n",
        "  X_train = text_seq[:test_size]\n",
        "  y_train = to_categorical(labels[:test_size], 2)\n",
        "  X_test = text_seq[test_size:]\n",
        "  y_test = to_categorical(labels[test_size:], 2)\n",
        "  \n",
        "  return X_train[:BATCH_SIZE], y_train[:BATCH_SIZE], X_test[:BATCH_SIZE], y_test[:BATCH_SIZE]\n",
        "  \n",
        "def get_evaluation_data():\n",
        "  global df_evaluation\n",
        "  df_evaluation = df_evaluation.sample(frac=1)\n",
        "  \n",
        "  texts, labels = load_data(df_evaluation)\n",
        "  return get_datasets(texts, labels)\n",
        "  \n",
        "def get_fake_data():\n",
        "  global df_fake\n",
        "  df_fake = df_fake.sample(frac=1)\n",
        "  \n",
        "  texts, labels = load_data(df_fake)\n",
        "  return get_datasets(texts, labels)\n",
        "\n",
        "def get_real_data():\n",
        "  global df_real\n",
        "  df_real = df_real.sample(frac=1)\n",
        "\n",
        "  texts, labels = load_data(df_real)\n",
        "  return get_datasets(texts, labels)\n",
        "\n",
        "def get_mixed_data():\n",
        "  # global notation ???\n",
        "  real = df_real.sample(frac=1)\n",
        "  fake = df_fake.sample(frac=1)\n",
        "  df = pd.concat([real[:BATCH_SIZE*6], fake[:BATCH_SIZE*6]])\n",
        "  df = df.sample(frac=1)\n",
        "  \n",
        "  texts, labels = load_data(df)\n",
        "  return get_datasets(texts, labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qIo4O_PW1Hfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discriminator\n",
        "model for classifying sequences (here headlines) as real or fake.\n",
        "In this implementation the discriminative model uses following layers: \n",
        "1.   embedding layer\n",
        "2.   convolution layer with max-pooling operation\n",
        "4.   softmax layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k3UdGGRc5FfF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator:\n",
        "  def __init__(self, batch_size, vocab_size, seq_length, emb_size, num_classes, \n",
        "               filter_sizes, num_filters, learning_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.seq_length = seq_length\n",
        "    self.emb_size = emb_size\n",
        "    self.num_classes = num_classes\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.num_filters = num_filters\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length], name='d_X_input')\n",
        "    self.y_input = tf.placeholder(tf.int32, shape=[self.batch_size, self.num_classes], name='d_y_input')\n",
        "    self.dropout_keep_prob = tf.placeholder(tf.float32, name='d_dropout_keep_prob')\n",
        "    \n",
        "    # Keeping track of l2 regularization loss (optional)\n",
        "    self.l2_reg_lambda = 0.2\n",
        "    self.l2_loss = tf.constant(0.0)\n",
        "\n",
        "  def build_model(self):\n",
        "    with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):\n",
        "      self.embedding_layer = self.build_embedding_layer()\n",
        "      self.convolution_maxpool_layer = self.build_convolution_maxpool_layer()\n",
        "      self.scores, self.predictions = self.build_softmax_layer()\n",
        "      \n",
        "      # PRETRAINING\n",
        "      self.pretrain_loss = self.calc_mean_cross_entropy_loss()\n",
        "      self.pretrain_accuracy = self.calc_accuracy()\n",
        "      self.pretrain_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.pretrain_loss)\n",
        "\n",
        "      # TRAINING\n",
        "      self.loss = self.calc_mean_cross_entropy_loss()\n",
        "      self.accuracy = self.calc_accuracy()\n",
        "      self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "      \n",
        "      # SUMMARY\n",
        "      self.d_pretrain_summary = tf.summary.merge([\n",
        "          tf.summary.scalar('d_pretrain_loss', self.pretrain_loss),\n",
        "          tf.summary.scalar('d_pretrain_accuracy', self.pretrain_accuracy)\n",
        "      ])\n",
        "      self.d_summary = tf.summary.merge([\n",
        "          tf.summary.scalar('d_loss', self.loss),\n",
        "          tf.summary.scalar('d_accuracy', self.accuracy)\n",
        "      ])\n",
        "        \n",
        "  def build_embedding_layer(self):\n",
        "    with tf.device('gpu:0'), tf.name_scope('d_embedding_layer'):\n",
        "      emb_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0))\n",
        "      emb_lookup = tf.nn.embedding_lookup(emb_matrix, self.X_input)\n",
        "      emb_lookup_expand = tf.expand_dims(emb_lookup, -1)\n",
        "      \n",
        "      return emb_lookup_expand\n",
        "    \n",
        "  def build_convolution_maxpool_layer(self):\n",
        "    with tf.name_scope('d_convolution_maxpool_layer'):\n",
        "      pooled_outputs = []\n",
        "      for filter_size in self.filter_sizes:\n",
        "        with tf.name_scope('d_conv-maxpool-%s' % filter_size):\n",
        "          # Convolution Layer\n",
        "          filter_shape = [filter_size, self.emb_size, 1, self.num_filters]\n",
        "          W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='d_W')\n",
        "          b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name='d_b')\n",
        "          conv = tf.nn.conv2d(\n",
        "              input=self.embedding_layer,\n",
        "              filter=W,\n",
        "              strides=[1,1,1,1], # the filter is applied to the input in one-pixel intervals in each dimension, corresponding to a “full” convolution\n",
        "              padding='VALID',\n",
        "              name='d_conv'\n",
        "          )\n",
        "          # Apply non-linearity - activation function\n",
        "          activation = tf.nn.relu(tf.nn.bias_add(conv, b), name='d_relu')\n",
        "          # Maxpooling over outputs\n",
        "          max_pooling = tf.nn.max_pool(\n",
        "              value=activation,\n",
        "              ksize=[1, self.seq_length-filter_size+1, 1, 1],\n",
        "              strides=[1,1,1,1],\n",
        "              padding='VALID',\n",
        "              name='max_pooling'\n",
        "          )\n",
        "          pooled_outputs.append(max_pooling)\n",
        "\n",
        "      # combine all the pooled features\n",
        "      self.num_filter_total = self.num_filters * len(self.filter_sizes)\n",
        "      h_pool = tf.concat(pooled_outputs, axis=3)\n",
        "      \n",
        "      return tf.reshape(h_pool, [-1, self.num_filter_total])\n",
        "        \n",
        "  def build_softmax_layer(self): \n",
        "    with tf.name_scope('highway'):\n",
        "      self.h_highway = self.highway(\n",
        "          self.convolution_maxpool_layer, self.convolution_maxpool_layer.get_shape()[1], 1, 0\n",
        "      )\n",
        "\n",
        "    with tf.name_scope('dropout'):\n",
        "      self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
        "      \n",
        "    with tf.name_scope('softmax_output'):\n",
        "      W_softmax = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              [self.num_filter_total, self.num_classes], \n",
        "              stddev=0.1\n",
        "          ), name='d_W_softmax'\n",
        "      )\n",
        "      b_softmax = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name='d_b_softmax')\n",
        "            \n",
        "      self.l2_loss += tf.nn.l2_loss(W_softmax)\n",
        "      self.l2_loss += tf.nn.l2_loss(b_softmax)\n",
        "      \n",
        "      self.scores = tf.nn.xw_plus_b(self.h_drop, W_softmax, b_softmax, name='d_scores')\n",
        "      #self.scores = tf.matmul(self.convolution_maxpool_layer, W_softmax) + b_softmax\n",
        "      self.truth_prob = tf.nn.softmax(self.scores, -1)[:, 1]\n",
        "      predictions = tf.argmax(self.scores, 1, name='d_predictions')\n",
        "      \n",
        "    return self.scores, predictions\n",
        "  \n",
        "  def calc_mean_cross_entropy_loss(self):\n",
        "    with tf.name_scope('d_loss'):\n",
        "      losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=self.y_input)\n",
        "      loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
        "      \n",
        "      return loss\n",
        "      \n",
        "  def calc_accuracy(self):\n",
        "    with tf.name_scope('d_accuracy'):\n",
        "      correct_predictions = tf.equal(self.predictions, tf.argmax(self.y_input, 1))\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
        "      \n",
        "      return accuracy\n",
        "      \n",
        "  def highway(self, input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n",
        "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
        "    t = sigmoid(Wy + b)\n",
        "    z = t * g(Wy + b) + (1 - t) * y\n",
        "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
        "    \"\"\"\n",
        "    with tf.variable_scope('highway'):\n",
        "      size = int(size)\n",
        "      for idx in range(num_layers):\n",
        "        g = f(slim.fully_connected(input_, size, scope='highway_lin_%d' % idx, activation_fn=None))\n",
        "        t = tf.sigmoid(slim.fully_connected(input_, size, scope='highway_gate_%d' % idx, activation_fn=None) + bias)\n",
        "\n",
        "        output = t * g + (1. - t) * input_\n",
        "        input_ = output\n",
        "        \n",
        "    return output\n",
        "  \n",
        "  def get_truth_prob(self, sess, X):\n",
        "    feed_dict = { self.X_input: X, self.dropout_keep_prob: 1.0 }\n",
        "        \n",
        "    return sess.run(self.truth_prob, feed_dict=feed_dict)\n",
        "  \n",
        "  def pretrain(self, sess, X, y, dropout):\n",
        "    feed_dict = {\n",
        "        self.X_input: X,\n",
        "        self.y_input: y,\n",
        "        self.dropout_keep_prob: dropout\n",
        "    }\n",
        "    _, summary, loss, acc = sess.run([\n",
        "        self.pretrain_optimizer, \n",
        "        self.d_pretrain_summary,\n",
        "        self.pretrain_loss,\n",
        "        self.pretrain_accuracy\n",
        "    ], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary, loss, acc\n",
        "        \n",
        "  def train(self, sess, X, y, dropout):\n",
        "    feed_dict = {\n",
        "        self.X_input: X,\n",
        "        self.y_input: y,\n",
        "        self.dropout_keep_prob: dropout\n",
        "    }\n",
        "    _, summary, loss, acc = sess.run([\n",
        "        self.optimizer, \n",
        "        self.d_summary,\n",
        "        self.loss,\n",
        "        self.accuracy\n",
        "    ], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary, loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B86yDcx3TC4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Discriminator model and train model"
      ]
    },
    {
      "metadata": {
        "id": "UnVPueM8TXwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "discriminator = Discriminator(BATCH_SIZE, VOCAB_SIZE, SEQ_LENGTH, D_EMB_SIZE, \n",
        "                              D_NUM_CLASSES, D_FILTER_SIZES, D_NUM_FILTERS, D_LEARNING_RATE)\n",
        "discriminator.build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmjkZBeH557j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "\n",
        "LSTM with Reinforcement Learning for sequence generation."
      ]
    },
    {
      "metadata": {
        "id": "lcALithWy39s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib import rnn, layers, seq2seq, slim\n",
        "import random\n",
        "\n",
        "# inspired by https://www.oreilly.com/ideas/introduction-to-lstms-with-tensorflow\n",
        "\n",
        "class Generator:\n",
        "  def __init__(self, batch_size, seq_length, vocab_size, emb_size, temperature,\n",
        "               hidden_layer_sizes, word_index, learning_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_length = seq_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_layer_sizes = hidden_layer_sizes\n",
        "    self.word_index = word_index\n",
        "    self.learning_rate = learning_rate\n",
        "    self.temperature = temperature\n",
        "    \n",
        "    self.grad_clip = 5.0\n",
        "    \n",
        "    self.X_input = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length], name='g_X_input')\n",
        "    self.dropout_keep_prob = tf.placeholder(tf.float32, name='g_dropout_keep_prob')\n",
        "    self.rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.seq_length], name='rewards')\n",
        "    \n",
        "  def build_model(self):\n",
        "    with tf.variable_scope('generator', reuse=tf.AUTO_REUSE):\n",
        "      self.embedding_layer = self.build_embedding_layer(self.X_input)\n",
        "      self.outputs, final_state = self.build_lstm_layers()\n",
        "      \n",
        "      # PRETRAINING\n",
        "      pretrain_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "      pretrain_predictions, self.pretrain_loss = self.get_prediction_and_loss()\n",
        "      self.pretrain_operation = slim.learning.create_train_op(\n",
        "          self.pretrain_loss, pretrain_optimizer, clip_gradient_norm=self.grad_clip\n",
        "      )\n",
        "      \n",
        "      # TRAINING\n",
        "      self.predictions, self.loss = self.get_prediction_and_loss()\n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "      self.train_operation = slim.learning.create_train_op(\n",
        "          self.loss, optimizer, clip_gradient_norm=self.grad_clip\n",
        "      )\n",
        "                  \n",
        "      # SUMMARY\n",
        "      self.g_pretrain_summary = tf.summary.scalar('g_pretrain_loss', self.pretrain_loss)\n",
        "      self.g_train_summary = tf.summary.merge([\n",
        "          tf.summary.scalar('g_loss', self.loss),\n",
        "          tf.summary.scalar('g_reward', tf.reduce_mean(self.rewards))\n",
        "      ])\n",
        "        \n",
        "  def build_embedding_layer(self, X_input):\n",
        "    with tf.name_scope('g_embedding_layer'):\n",
        "      emb_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0))\n",
        "      emb_lookup = tf.nn.embedding_lookup(emb_matrix, X_input)\n",
        "      \n",
        "    return emb_lookup\n",
        "      \n",
        "  def build_lstm_layers(self):\n",
        "    with tf.name_scope('g_lstm_layers'):\n",
        "      layers = [rnn.LSTMCell(layer_size) for layer_size in self.hidden_layer_sizes]\n",
        "      dropouts = [rnn.DropoutWrapper(layer, output_keep_prob=self.dropout_keep_prob) for layer in layers]\n",
        "      cell = rnn.MultiRNNCell(dropouts) # , state_is_tuple=True)?\n",
        "      \n",
        "      initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
        "      outputs, final_state = tf.nn.dynamic_rnn(cell, self.embedding_layer, initial_state=initial_state)\n",
        "      \n",
        "    return outputs, final_state\n",
        "  \n",
        "  def get_prediction_and_loss(self):\n",
        "    predictions = []\n",
        "    W2 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[-1], self.vocab_size]), dtype=tf.float32)\n",
        "    b2 = tf.Variable(tf.zeros([1, self.vocab_size]), dtype=tf.float32)\n",
        "      \n",
        "    output = tf.reshape(self.outputs, [-1, self.hidden_layer_sizes[-1]])\n",
        "    logits = tf.matmul(output, W2) + b2 # broadcasted addition\n",
        "    predictions.append(tf.nn.softmax(tf.divide(logits, self.temperature)))\n",
        "      \n",
        "    loss = -tf.reduce_sum(\n",
        "      tf.one_hot(tf.to_int32(tf.reshape(self.X_input, [-1])), self.vocab_size, 1.0, 0.0) * \n",
        "      tf.log(tf.clip_by_value(tf.reshape(predictions[-1], [-1, self.vocab_size]), 1e-20, 1.0))\n",
        "    ) / (self.seq_length * self.batch_size)\n",
        "    \n",
        "    return predictions, loss\n",
        "      \n",
        "  def generate(self, sess, given_tokens, dropout):\n",
        "    feed_dict = { self.X_input: given_tokens, self.dropout_keep_prob: dropout }\n",
        "    samples = sess.run([self.predictions], feed_dict=feed_dict)\n",
        "    sentences, sequences = self.translate_samples(samples)\n",
        "    \n",
        "    return sentences, sequences\n",
        "    \n",
        "  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "  # REINFORCEMENT LEARNING  \n",
        "  def get_reward(self, sess, given_tokens, rollout_num, dis, dropout):\n",
        "    rewards = np.zeros((self.batch_size, self.seq_length))\n",
        "        \n",
        "    for i in range(rollout_num):\n",
        "      for given_num in range(1, self.seq_length):\n",
        "        feed_dict = { self.X_input: given_tokens, self.dropout_keep_prob: dropout }\n",
        "        samples = sess.run(self.predictions, feed_dict=feed_dict)\n",
        "        sentences, sequence = self.translate_samples(samples)\n",
        "        rewards[:, given_num] += dis.get_truth_prob(sess, sequence)\n",
        "            \n",
        "    rewards /= rollout_num\n",
        "    return rewards\n",
        "  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "  \n",
        "  def pretrain(self, sess, given_tokens, dropout):\n",
        "    feed_dict = { self.X_input: given_tokens, self.dropout_keep_prob: dropout }\n",
        "    _, summary, pretrain_loss = sess.run([\n",
        "        self.pretrain_operation, \n",
        "        self.g_pretrain_summary,\n",
        "        self.pretrain_loss\n",
        "    ], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary, pretrain_loss\n",
        "  \n",
        "  def train(self, sess, given_tokens, rewards, dropout):\n",
        "    feed_dict = { self.X_input: given_tokens, self.rewards: rewards, self.dropout_keep_prob: dropout }\n",
        "    _, summary, loss, rewards = sess.run([\n",
        "        self.train_operation, \n",
        "        self.g_train_summary, \n",
        "        self.loss, \n",
        "        tf.reduce_mean(self.rewards)\n",
        "    ], feed_dict=feed_dict)\n",
        "    \n",
        "    return summary, loss, rewards\n",
        "  \n",
        "  def translate_samples(self, sequence):\n",
        "    batch_softmax = np.reshape(sequence, [self.batch_size, self.seq_length, self.vocab_size])\n",
        "\n",
        "    sentences = []\n",
        "    vectors = []\n",
        "    for sequence in batch_softmax:\n",
        "      sentence = ''\n",
        "      vector = []\n",
        "      for pos in sequence:\n",
        "        vector_position = np.argmax(pos)\n",
        "        vector.append(vector_position)\n",
        "        word = self.word_index[vector_position]\n",
        "        sentence += word\n",
        "        sentence += ' '\n",
        "\n",
        "      sentences.append(sentence)\n",
        "      vectors.append(vector)\n",
        "\n",
        "    vectors = np.asarray(vectors)\n",
        "    return sentences, vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V03XEe_uVdwq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Generator model"
      ]
    },
    {
      "metadata": {
        "id": "PW7Hyz2J6jro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = Generator(BATCH_SIZE, SEQ_LENGTH, VOCAB_SIZE, G_EMB_SIZE, G_TEMPERATURE,\n",
        "                      G_HIDDEN_LAYER_SIZES, WORD_INDEX, G_LEARNING_RATE)\n",
        "\n",
        "target_lstm = Generator(BATCH_SIZE, SEQ_LENGTH, VOCAB_SIZE, G_EMB_SIZE, G_TEMPERATURE,\n",
        "                        G_HIDDEN_LAYER_SIZES, WORD_INDEX, G_LEARNING_RATE)\n",
        "\n",
        "generator.build_model()\n",
        "target_lstm.build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uccb8c0sViK5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Start Adversarial Training"
      ]
    },
    {
      "metadata": {
        "id": "hN-drlXj1Kyj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tbc=TensorBoardColab()\n",
        "\n",
        "LOG_DIR = '/tmp/log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ymHD3nwRtS7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def target_loss(sess, target_lstm, X_train):\n",
        "  # target_loss means the oracle negative log-likelihood tested with the oracle model \"target_lstm\"\n",
        "  # For more details, please see the Section 4 in https://arxiv.org/abs/1609.05473\n",
        "  nll = []\n",
        "\n",
        "  for _ in range(BATCH_SIZE):\n",
        "    fake_sentences, fake_sequences = target_lstm.generate(sess, X_train, G_DROPOUT)\n",
        "    summary, g_pretrain_loss = target_lstm.pretrain(sess, X_train, G_DROPOUT)\n",
        "    nll.append(g_pretrain_loss)\n",
        "\n",
        "  return np.mean(nll)\n",
        "\n",
        "def add_samples(headlines, df):\n",
        "  for headline in headlines:\n",
        "    df = df.append({'text': headline , 'fake': 1} , ignore_index=True)\n",
        "    \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GbdSazLhVllp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  random.seed(88)\n",
        "  np.random.seed(88)\n",
        "        \n",
        "  print('start GAN training at', datetime.datetime.now())\n",
        "  writer = tbc.get_writer()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  # PRETRAINING GENERATOR\n",
        "  for epoch in tnrange(G_PRETRAIN_EPOCHS, desc='gen_pretrain_loop'):\n",
        "    X_train, y_train, X_test, y_test = get_real_data()\n",
        "    summary, g_pretrain_loss = generator.pretrain(sess, X_train, G_DROPOUT)\n",
        "    if epoch % 5 == 0:\n",
        "      fake_sentences, fake_sequences = generator.generate(sess, X_train, G_DROPOUT)\n",
        "      df_evaluation = add_samples(fake_sentences, df_evaluation)\n",
        "      X_train, y_train, X_test, y_test = get_evaluation_data()\n",
        "      test_loss = target_loss(sess, target_lstm, X_train)\n",
        "      print('target_lstm_pretrain_summary: nll={0:.3f}'.format(test_loss))\n",
        "      \n",
        "    writer.add_summary(summary, epoch)\n",
        "    print('g_pretrain_summary: pretrain_loss={0:.3f}'.format(g_pretrain_loss))\n",
        "    \n",
        "  # PRETRAIN DISCRIMINATOR\n",
        "  for epoch in tnrange(D_PRETRAIN_EPOCHS, desc='dis_pretrain_loop'):\n",
        "    X_train, y_train, X_test, y_test = get_real_data()\n",
        "    fake_sentences, fake_sequences = generator.generate(sess, X_train, G_DROPOUT)\n",
        "    df_fake = add_samples(fake_sentences, df_fake)\n",
        "    \n",
        "    for _ in range(3):\n",
        "      for _ in range(BATCH_SIZE):\n",
        "        X_train, y_train, X_test, y_test = get_mixed_data() #maybe out of this loop?\n",
        "        summary, d_pretrain_loss, d_pretrain_acc = discriminator.pretrain(sess, X_train, y_train, D_DROPOUT)\n",
        "      \n",
        "    writer.add_summary(summary, epoch)\n",
        "    print('d_pretrain_summary: d_pretrain_loss={0:.3f}, d_pretrain_accuracy={1:.3f}'.format(d_pretrain_loss, d_pretrain_acc))\n",
        "    \n",
        "  rollout = generator\n",
        "\n",
        "  # ADVERSARIAL TRAINING\n",
        "  for epoch in tnrange(TOTAL_EPOCHS, desc='gan_epoch_loop'):\n",
        "    X_train, y_train, X_test, y_test = get_real_data()\n",
        "    fake_sentences, fake_sequences = generator.generate(sess, X_train, G_DROPOUT)\n",
        "    df_fake = add_samples(fake_sentences, df_fake)\n",
        "    \n",
        "    rewards = rollout.get_reward(sess, fake_sequences, 16, discriminator, G_DROPOUT)\n",
        "    summary, g_loss, reward = generator.train(sess, fake_sequences, rewards, G_DROPOUT)\n",
        "    writer.add_summary(summary, epoch)\n",
        "    \n",
        "    print('g_summary: loss={0:.3f}, reward={1:.3f}'.format(g_loss, reward))\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "      fake_sentences, fake_sequences = generator.generate(sess, X_train, G_DROPOUT)\n",
        "      df_evaluation = add_samples(fake_sentences, df_evaluation)\n",
        "      X_train, y_train, X_test, y_test = get_evaluation_data()\n",
        "      test_loss = target_loss(sess, target_lstm, X_train)\n",
        "      print('target_lstm_summary: nll={0:.3f}'.format(test_loss))\n",
        "    \n",
        "    for _ in tnrange(G_EPOCHS, desc='gen_train_loop'):\n",
        "      X_train, y_train, X_test, y_test = get_real_data()\n",
        "      fake_sentences, fake_sequences = generator.generate(sess, X_train, G_DROPOUT)\n",
        "      df_fake = add_samples(fake_sentences, df_fake)\n",
        "      \n",
        "      for _ in tnrange(D_EPOCHS, desc='dis_train_loop'):\n",
        "        for _ in range(BATCH_SIZE):\n",
        "          X_train, y_train, X_test, y_test = get_mixed_data()\n",
        "          summary, d_loss, d_acc = discriminator.train(sess, X_train, y_train, D_DROPOUT)\n",
        "        \n",
        "      print('d_summary: loss={0:.3f}, accuracy={1:.3f}'.format(d_loss, d_acc))\n",
        "        \n",
        "    writer.add_summary(summary, epoch)\n",
        "    print('df_fake: ', df_fake.tail(2))\n",
        "    print('df_evaluation: ', df_evaluation.tail(2))\n",
        "    \n",
        "  print('finish GAN training at', datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2TgllHrMIdaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "print(df_fake.tail())\n",
        "\n",
        "count = 1\n",
        "saved = False\n",
        "while saved != True:\n",
        "  filename = 'generated_headlines_%d.csv' % count\n",
        "  filepath = Path('./results/%s' % filename)\n",
        "  \n",
        "  if filepath.exists():\n",
        "    count = count+1\n",
        "  else:\n",
        "    df_fake.to_csv(filepath, sep='\\t', encoding='utf-8')\n",
        "    saved = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_zdt1qf3Qa_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}